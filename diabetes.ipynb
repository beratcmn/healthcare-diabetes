{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZq/gCCk9v1HZ15TnVgoKy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beratcmn/healthcare-diabetes/blob/main/diabetes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9fqN1TbRgGr",
        "outputId": "ef4d5d0c-7755-4121-d822-10e8ae1f213e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Ic301xEPSJrv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"./Healthcare-Diabetes.csv\")"
      ],
      "metadata": {
        "id": "KXLuGrKmSLIg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POAoJwyRd8hT",
        "outputId": "a39a1baf-6b26-411d-e58e-9e8f5ae2b4bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2768 entries, 0 to 2767\n",
            "Data columns (total 10 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   Id                        2768 non-null   int64  \n",
            " 1   Pregnancies               2768 non-null   int64  \n",
            " 2   Glucose                   2768 non-null   int64  \n",
            " 3   BloodPressure             2768 non-null   int64  \n",
            " 4   SkinThickness             2768 non-null   int64  \n",
            " 5   Insulin                   2768 non-null   int64  \n",
            " 6   BMI                       2768 non-null   float64\n",
            " 7   DiabetesPedigreeFunction  2768 non-null   float64\n",
            " 8   Age                       2768 non-null   int64  \n",
            " 9   Outcome                   2768 non-null   int64  \n",
            "dtypes: float64(2), int64(8)\n",
            "memory usage: 216.4 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlTet0zCeVux",
        "outputId": "872919ca-acef-4622-a041-ed83864908d7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Id                          0\n",
              "Pregnancies                 0\n",
              "Glucose                     0\n",
              "BloodPressure               0\n",
              "SkinThickness               0\n",
              "Insulin                     0\n",
              "BMI                         0\n",
              "DiabetesPedigreeFunction    0\n",
              "Age                         0\n",
              "Outcome                     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.duplicated().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXiyfR94eb7_",
        "outputId": "19360e7d-a5fb-44dc-e2ea-90ac028bbb1f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"Outcome\"].value_counts().plot(kind=\"bar\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "LEuuSjq9eJ01",
        "outputId": "d7a71e21-2c47-42db-c506-8a68d66d27be"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop(columns=['Id'])"
      ],
      "metadata": {
        "id": "Evt3rKsmXEY1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "hURUkc-BSZlI",
        "outputId": "d8f99fbf-b8af-443d-f7a3-b814d9ddf38d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0            6      148             72             35        0  33.6   \n",
              "1            1       85             66             29        0  26.6   \n",
              "2            8      183             64              0        0  23.3   \n",
              "3            1       89             66             23       94  28.1   \n",
              "4            0      137             40             35      168  43.1   \n",
              "\n",
              "   DiabetesPedigreeFunction  Age  Outcome  \n",
              "0                     0.627   50        1  \n",
              "1                     0.351   31        0  \n",
              "2                     0.672   32        1  \n",
              "3                     0.167   21        0  \n",
              "4                     2.288   33        1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-223ef5f1-ac5e-4b7d-88a5-316144928a1c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-223ef5f1-ac5e-4b7d-88a5-316144928a1c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-223ef5f1-ac5e-4b7d-88a5-316144928a1c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-223ef5f1-ac5e-4b7d-88a5-316144928a1c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b2a18f79-7a65-4007-aef2-dcfc7024e950\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b2a18f79-7a65-4007-aef2-dcfc7024e950')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b2a18f79-7a65-4007-aef2-dcfc7024e950 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.sample(frac=1, random_state=36)"
      ],
      "metadata": {
        "id": "yuUTyyBXSdoc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop(columns=['Outcome'])\n",
        "y = data['Outcome']"
      ],
      "metadata": {
        "id": "axeA_rSYTuj6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "03tykmqUT22w",
        "outputId": "6c9e7207-227f-4ee3-e1bf-3f35572caa05"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "2307            3      158             70             30      328  35.5   \n",
              "2296            6      154             78             41      140  46.1   \n",
              "2001           10      161             68             23      132  25.5   \n",
              "2593            8       65             72             23        0  32.0   \n",
              "2424            9       72             78             25        0  31.6   \n",
              "\n",
              "      DiabetesPedigreeFunction  Age  \n",
              "2307                     0.344   35  \n",
              "2296                     0.571   27  \n",
              "2001                     0.326   47  \n",
              "2593                     0.600   42  \n",
              "2424                     0.280   38  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-244738a8-c61d-4f73-96dc-6af70af3c4e6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2307</th>\n",
              "      <td>3</td>\n",
              "      <td>158</td>\n",
              "      <td>70</td>\n",
              "      <td>30</td>\n",
              "      <td>328</td>\n",
              "      <td>35.5</td>\n",
              "      <td>0.344</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2296</th>\n",
              "      <td>6</td>\n",
              "      <td>154</td>\n",
              "      <td>78</td>\n",
              "      <td>41</td>\n",
              "      <td>140</td>\n",
              "      <td>46.1</td>\n",
              "      <td>0.571</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2001</th>\n",
              "      <td>10</td>\n",
              "      <td>161</td>\n",
              "      <td>68</td>\n",
              "      <td>23</td>\n",
              "      <td>132</td>\n",
              "      <td>25.5</td>\n",
              "      <td>0.326</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2593</th>\n",
              "      <td>8</td>\n",
              "      <td>65</td>\n",
              "      <td>72</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.600</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2424</th>\n",
              "      <td>9</td>\n",
              "      <td>72</td>\n",
              "      <td>78</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>31.6</td>\n",
              "      <td>0.280</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-244738a8-c61d-4f73-96dc-6af70af3c4e6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-244738a8-c61d-4f73-96dc-6af70af3c4e6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-244738a8-c61d-4f73-96dc-6af70af3c4e6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-36637902-4a89-4141-ad82-297a1714ba7c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-36637902-4a89-4141-ad82-297a1714ba7c')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-36637902-4a89-4141-ad82-297a1714ba7c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1AwsdXLT4Rg",
        "outputId": "cb153099-4c0f-4934-d142-3d3ec21731eb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2307    1\n",
              "2296    0\n",
              "2001    1\n",
              "2593    0\n",
              "2424    0\n",
              "Name: Outcome, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_proportion = 0.8"
      ],
      "metadata": {
        "id": "p_73lU_QUF6g"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_rows = int(train_proportion * len(X))\n",
        "X_test_rows = len(X) - X_train_rows\n",
        "X_train_rows, X_test_rows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lwmfOBkUMEo",
        "outputId": "c6d224a6-7eaa-4ca9-b31a-ca213ac1c2c8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2214, 554)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_rows = int(train_proportion * len(y))\n",
        "y_test_rows = len(y) - y_train_rows\n",
        "y_train_rows, y_test_rows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGtju-VsVoY2",
        "outputId": "a3524477-7ab0-4785-c7f9-7d99c8c4a329"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2214, 554)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X[:X_train_rows]\n",
        "X_test = X[X_test_rows:]"
      ],
      "metadata": {
        "id": "Rz8vFUvKUd8e"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "cU3-nq30Ussq",
        "outputId": "0440b838-af9a-4500-d849-09e25784b1ab"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "2307            3      158             70             30      328  35.5   \n",
              "2296            6      154             78             41      140  46.1   \n",
              "2001           10      161             68             23      132  25.5   \n",
              "2593            8       65             72             23        0  32.0   \n",
              "2424            9       72             78             25        0  31.6   \n",
              "\n",
              "      DiabetesPedigreeFunction  Age  \n",
              "2307                     0.344   35  \n",
              "2296                     0.571   27  \n",
              "2001                     0.326   47  \n",
              "2593                     0.600   42  \n",
              "2424                     0.280   38  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-49ad7df4-fcee-4816-92cd-6892dfac4242\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2307</th>\n",
              "      <td>3</td>\n",
              "      <td>158</td>\n",
              "      <td>70</td>\n",
              "      <td>30</td>\n",
              "      <td>328</td>\n",
              "      <td>35.5</td>\n",
              "      <td>0.344</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2296</th>\n",
              "      <td>6</td>\n",
              "      <td>154</td>\n",
              "      <td>78</td>\n",
              "      <td>41</td>\n",
              "      <td>140</td>\n",
              "      <td>46.1</td>\n",
              "      <td>0.571</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2001</th>\n",
              "      <td>10</td>\n",
              "      <td>161</td>\n",
              "      <td>68</td>\n",
              "      <td>23</td>\n",
              "      <td>132</td>\n",
              "      <td>25.5</td>\n",
              "      <td>0.326</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2593</th>\n",
              "      <td>8</td>\n",
              "      <td>65</td>\n",
              "      <td>72</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.600</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2424</th>\n",
              "      <td>9</td>\n",
              "      <td>72</td>\n",
              "      <td>78</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>31.6</td>\n",
              "      <td>0.280</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49ad7df4-fcee-4816-92cd-6892dfac4242')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-49ad7df4-fcee-4816-92cd-6892dfac4242 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-49ad7df4-fcee-4816-92cd-6892dfac4242');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1677fdff-8959-4535-80eb-659d95f75b4b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1677fdff-8959-4535-80eb-659d95f75b4b')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1677fdff-8959-4535-80eb-659d95f75b4b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "VpGb29J3Uv4y",
        "outputId": "88311ce5-2d23-47ee-8842-26979d23143d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "1284            9      145             88             34      165  30.3   \n",
              "2011            3       99             80             11       64  19.3   \n",
              "1271            7       94             64             25       79  33.3   \n",
              "1392            2      108             64              0        0  30.8   \n",
              "1278           12       84             72             31        0  29.7   \n",
              "\n",
              "      DiabetesPedigreeFunction  Age  \n",
              "1284                     0.771   53  \n",
              "2011                     0.284   30  \n",
              "1271                     0.738   41  \n",
              "1392                     0.158   21  \n",
              "1278                     0.297   46  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fbd9e7c4-1828-4b5f-a970-cba21c36cc32\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1284</th>\n",
              "      <td>9</td>\n",
              "      <td>145</td>\n",
              "      <td>88</td>\n",
              "      <td>34</td>\n",
              "      <td>165</td>\n",
              "      <td>30.3</td>\n",
              "      <td>0.771</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2011</th>\n",
              "      <td>3</td>\n",
              "      <td>99</td>\n",
              "      <td>80</td>\n",
              "      <td>11</td>\n",
              "      <td>64</td>\n",
              "      <td>19.3</td>\n",
              "      <td>0.284</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1271</th>\n",
              "      <td>7</td>\n",
              "      <td>94</td>\n",
              "      <td>64</td>\n",
              "      <td>25</td>\n",
              "      <td>79</td>\n",
              "      <td>33.3</td>\n",
              "      <td>0.738</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1392</th>\n",
              "      <td>2</td>\n",
              "      <td>108</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.8</td>\n",
              "      <td>0.158</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1278</th>\n",
              "      <td>12</td>\n",
              "      <td>84</td>\n",
              "      <td>72</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>29.7</td>\n",
              "      <td>0.297</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fbd9e7c4-1828-4b5f-a970-cba21c36cc32')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fbd9e7c4-1828-4b5f-a970-cba21c36cc32 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fbd9e7c4-1828-4b5f-a970-cba21c36cc32');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1f6efe3c-8988-4aaf-b888-0c5e15e945bb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1f6efe3c-8988-4aaf-b888-0c5e15e945bb')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1f6efe3c-8988-4aaf-b888-0c5e15e945bb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y[:y_train_rows]\n",
        "y_test = y[y_test_rows:]"
      ],
      "metadata": {
        "id": "fWmh8q-sVsKH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUqh-bPfVubr",
        "outputId": "41e74e1b-ad42-4508-b6ea-98d1bf93f747"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2307    1\n",
              "2296    0\n",
              "2001    1\n",
              "2593    0\n",
              "2424    0\n",
              "Name: Outcome, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Dmy3mPrVvSg",
        "outputId": "ae1e7880-8864-432a-93ae-66eba41a4021"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1284    1\n",
              "2011    0\n",
              "1271    0\n",
              "1392    0\n",
              "1278    1\n",
              "Name: Outcome, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXKobmVCWtK6",
        "outputId": "635c06a1-2aaa-4fef-fb92-b6800eee252d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2214, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = None"
      ],
      "metadata": {
        "id": "oY1rRXF1ZRDi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Normalization(),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upAnqjqPV1Sf",
        "outputId": "6d45ee51-a933-4948-8578-2dd1992239da"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " normalization (Normalizatio  (None, 8)                17        \n",
            " n)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               2304      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                528       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 46,098\n",
            "Trainable params: 46,081\n",
            "Non-trainable params: 17\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "f1wxVNIHYXqO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='model.h5',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    verbose=1,\n",
        ")"
      ],
      "metadata": {
        "id": "yr6u1h5yjWlN"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=16, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "LXdzoGy6kXiA"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs=512, batch_size=128, validation_split=0.1, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U8xnroOYdOP",
        "outputId": "998dc1c3-b4a4-4c09-96c5-f89b90ad1804"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.1991 - accuracy: 0.5797\n",
            "Epoch 1: val_accuracy improved from -inf to 0.68018, saving model to model.h5\n",
            "16/16 [==============================] - 5s 54ms/step - loss: 1.1826 - accuracy: 0.5818 - val_loss: 0.6269 - val_accuracy: 0.6802\n",
            "Epoch 2/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6524 - accuracy: 0.6521\n",
            "Epoch 2: val_accuracy did not improve from 0.68018\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.6502 - accuracy: 0.6521 - val_loss: 0.6032 - val_accuracy: 0.6622\n",
            "Epoch 3/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.6158 - accuracy: 0.6927\n",
            "Epoch 3: val_accuracy did not improve from 0.68018\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.6145 - accuracy: 0.6817 - val_loss: 0.5986 - val_accuracy: 0.6802\n",
            "Epoch 4/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.6161 - accuracy: 0.6911\n",
            "Epoch 4: val_accuracy improved from 0.68018 to 0.73874, saving model to model.h5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.6209 - accuracy: 0.6792 - val_loss: 0.5725 - val_accuracy: 0.7387\n",
            "Epoch 5/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5789 - accuracy: 0.6906\n",
            "Epoch 5: val_accuracy did not improve from 0.73874\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.5786 - accuracy: 0.6953 - val_loss: 0.5752 - val_accuracy: 0.7027\n",
            "Epoch 6/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.5698 - accuracy: 0.7079\n",
            "Epoch 6: val_accuracy did not improve from 0.73874\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.5664 - accuracy: 0.7088 - val_loss: 0.5566 - val_accuracy: 0.7027\n",
            "Epoch 7/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5537 - accuracy: 0.7276\n",
            "Epoch 7: val_accuracy did not improve from 0.73874\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.5515 - accuracy: 0.7279 - val_loss: 0.5729 - val_accuracy: 0.6982\n",
            "Epoch 8/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.5564 - accuracy: 0.7163\n",
            "Epoch 8: val_accuracy did not improve from 0.73874\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.5643 - accuracy: 0.7093 - val_loss: 0.5491 - val_accuracy: 0.7297\n",
            "Epoch 9/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.5508 - accuracy: 0.7218\n",
            "Epoch 9: val_accuracy improved from 0.73874 to 0.78378, saving model to model.h5\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.5428 - accuracy: 0.7269 - val_loss: 0.5145 - val_accuracy: 0.7838\n",
            "Epoch 10/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5424 - accuracy: 0.7179\n",
            "Epoch 10: val_accuracy improved from 0.78378 to 0.80180, saving model to model.h5\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.5424 - accuracy: 0.7179 - val_loss: 0.5000 - val_accuracy: 0.8018\n",
            "Epoch 11/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5273 - accuracy: 0.7259\n",
            "Epoch 11: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.5273 - accuracy: 0.7259 - val_loss: 0.5143 - val_accuracy: 0.7342\n",
            "Epoch 12/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.5131 - accuracy: 0.7528\n",
            "Epoch 12: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.5236 - accuracy: 0.7465 - val_loss: 0.5128 - val_accuracy: 0.7568\n",
            "Epoch 13/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.5364 - accuracy: 0.7320\n",
            "Epoch 13: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.5306 - accuracy: 0.7395 - val_loss: 0.4947 - val_accuracy: 0.7703\n",
            "Epoch 14/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5127 - accuracy: 0.7583\n",
            "Epoch 14: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.5143 - accuracy: 0.7565 - val_loss: 0.4928 - val_accuracy: 0.7928\n",
            "Epoch 15/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4964 - accuracy: 0.7631\n",
            "Epoch 15: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.4964 - accuracy: 0.7631 - val_loss: 0.5165 - val_accuracy: 0.7297\n",
            "Epoch 16/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.5024 - accuracy: 0.7569\n",
            "Epoch 16: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.4940 - accuracy: 0.7615 - val_loss: 0.5433 - val_accuracy: 0.7117\n",
            "Epoch 17/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.5281 - accuracy: 0.7461\n",
            "Epoch 17: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.5201 - accuracy: 0.7500 - val_loss: 0.5248 - val_accuracy: 0.7252\n",
            "Epoch 18/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.5102 - accuracy: 0.7461\n",
            "Epoch 18: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.4993 - accuracy: 0.7505 - val_loss: 0.4866 - val_accuracy: 0.7793\n",
            "Epoch 19/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5266 - accuracy: 0.7333\n",
            "Epoch 19: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.5270 - accuracy: 0.7324 - val_loss: 0.5182 - val_accuracy: 0.7748\n",
            "Epoch 20/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5069 - accuracy: 0.7600\n",
            "Epoch 20: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.5069 - accuracy: 0.7600 - val_loss: 0.5247 - val_accuracy: 0.7613\n",
            "Epoch 21/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.4967 - accuracy: 0.7630\n",
            "Epoch 21: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.4972 - accuracy: 0.7570 - val_loss: 0.5130 - val_accuracy: 0.7658\n",
            "Epoch 22/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4815 - accuracy: 0.7721\n",
            "Epoch 22: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.4815 - accuracy: 0.7721 - val_loss: 0.4732 - val_accuracy: 0.7973\n",
            "Epoch 23/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.4831 - accuracy: 0.7656\n",
            "Epoch 23: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.4840 - accuracy: 0.7626 - val_loss: 0.4967 - val_accuracy: 0.7477\n",
            "Epoch 24/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.4556 - accuracy: 0.8003\n",
            "Epoch 24: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.4645 - accuracy: 0.7851 - val_loss: 0.5088 - val_accuracy: 0.7568\n",
            "Epoch 25/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.4674 - accuracy: 0.7865\n",
            "Epoch 25: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.4700 - accuracy: 0.7836 - val_loss: 0.4858 - val_accuracy: 0.7658\n",
            "Epoch 26/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.4672 - accuracy: 0.7789\n",
            "Epoch 26: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.4687 - accuracy: 0.7806 - val_loss: 0.4548 - val_accuracy: 0.7973\n",
            "Epoch 27/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.4587 - accuracy: 0.7792\n",
            "Epoch 27: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.4650 - accuracy: 0.7746 - val_loss: 0.4694 - val_accuracy: 0.7748\n",
            "Epoch 28/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.4647 - accuracy: 0.7797\n",
            "Epoch 28: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.4651 - accuracy: 0.7801 - val_loss: 0.5255 - val_accuracy: 0.7523\n",
            "Epoch 29/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.4830 - accuracy: 0.7670\n",
            "Epoch 29: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.4844 - accuracy: 0.7641 - val_loss: 0.6015 - val_accuracy: 0.6757\n",
            "Epoch 30/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.4524 - accuracy: 0.7856\n",
            "Epoch 30: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.4811 - accuracy: 0.7666 - val_loss: 0.5193 - val_accuracy: 0.7252\n",
            "Epoch 31/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.4606 - accuracy: 0.7721\n",
            "Epoch 31: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.4699 - accuracy: 0.7671 - val_loss: 0.5207 - val_accuracy: 0.6982\n",
            "Epoch 32/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.4855 - accuracy: 0.7706\n",
            "Epoch 32: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.4835 - accuracy: 0.7721 - val_loss: 0.4920 - val_accuracy: 0.7613\n",
            "Epoch 33/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.4694 - accuracy: 0.7773\n",
            "Epoch 33: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.4628 - accuracy: 0.7766 - val_loss: 0.4648 - val_accuracy: 0.7658\n",
            "Epoch 34/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.4871 - accuracy: 0.7637\n",
            "Epoch 34: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.4947 - accuracy: 0.7575 - val_loss: 0.4836 - val_accuracy: 0.7748\n",
            "Epoch 35/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.4438 - accuracy: 0.7912\n",
            "Epoch 35: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.4443 - accuracy: 0.7887 - val_loss: 0.4597 - val_accuracy: 0.7703\n",
            "Epoch 36/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.4339 - accuracy: 0.7935\n",
            "Epoch 36: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.4446 - accuracy: 0.7866 - val_loss: 0.4555 - val_accuracy: 0.7793\n",
            "Epoch 37/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.4687 - accuracy: 0.7682\n",
            "Epoch 37: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.4649 - accuracy: 0.7721 - val_loss: 0.5044 - val_accuracy: 0.7658\n",
            "Epoch 38/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.4406 - accuracy: 0.7985\n",
            "Epoch 38: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.4394 - accuracy: 0.8002 - val_loss: 0.4515 - val_accuracy: 0.7613\n",
            "Epoch 39/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.4320 - accuracy: 0.7981\n",
            "Epoch 39: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.4306 - accuracy: 0.7977 - val_loss: 0.4426 - val_accuracy: 0.7883\n",
            "Epoch 40/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.4430 - accuracy: 0.7867\n",
            "Epoch 40: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.4426 - accuracy: 0.7871 - val_loss: 0.4537 - val_accuracy: 0.7973\n",
            "Epoch 41/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.4197 - accuracy: 0.8021\n",
            "Epoch 41: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.4371 - accuracy: 0.7922 - val_loss: 0.4544 - val_accuracy: 0.7838\n",
            "Epoch 42/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.4309 - accuracy: 0.7948\n",
            "Epoch 42: val_accuracy did not improve from 0.80180\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.4298 - accuracy: 0.7957 - val_loss: 0.4619 - val_accuracy: 0.7658\n",
            "Epoch 43/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.4172 - accuracy: 0.8011\n",
            "Epoch 43: val_accuracy improved from 0.80180 to 0.81081, saving model to model.h5\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.4182 - accuracy: 0.8002 - val_loss: 0.4326 - val_accuracy: 0.8108\n",
            "Epoch 44/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.4196 - accuracy: 0.8097\n",
            "Epoch 44: val_accuracy did not improve from 0.81081\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.4261 - accuracy: 0.8062 - val_loss: 0.4832 - val_accuracy: 0.7613\n",
            "Epoch 45/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.4108 - accuracy: 0.8119\n",
            "Epoch 45: val_accuracy did not improve from 0.81081\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.4087 - accuracy: 0.8163 - val_loss: 0.4363 - val_accuracy: 0.8063\n",
            "Epoch 46/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.4121 - accuracy: 0.8172\n",
            "Epoch 46: val_accuracy did not improve from 0.81081\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.4047 - accuracy: 0.8238 - val_loss: 0.4424 - val_accuracy: 0.8018\n",
            "Epoch 47/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.4070 - accuracy: 0.8002\n",
            "Epoch 47: val_accuracy did not improve from 0.81081\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.4030 - accuracy: 0.8052 - val_loss: 0.4443 - val_accuracy: 0.7793\n",
            "Epoch 48/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4012 - accuracy: 0.8153\n",
            "Epoch 48: val_accuracy did not improve from 0.81081\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.4012 - accuracy: 0.8153 - val_loss: 0.4798 - val_accuracy: 0.7568\n",
            "Epoch 49/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.3852 - accuracy: 0.8234\n",
            "Epoch 49: val_accuracy did not improve from 0.81081\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.3840 - accuracy: 0.8238 - val_loss: 0.4359 - val_accuracy: 0.7748\n",
            "Epoch 50/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.3972 - accuracy: 0.8147\n",
            "Epoch 50: val_accuracy did not improve from 0.81081\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.3993 - accuracy: 0.8122 - val_loss: 0.4383 - val_accuracy: 0.7838\n",
            "Epoch 51/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.3809 - accuracy: 0.8338\n",
            "Epoch 51: val_accuracy did not improve from 0.81081\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.3833 - accuracy: 0.8273 - val_loss: 0.4245 - val_accuracy: 0.7703\n",
            "Epoch 52/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.3797 - accuracy: 0.8292\n",
            "Epoch 52: val_accuracy did not improve from 0.81081\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.3812 - accuracy: 0.8263 - val_loss: 0.4521 - val_accuracy: 0.7883\n",
            "Epoch 53/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.3856 - accuracy: 0.8217\n",
            "Epoch 53: val_accuracy did not improve from 0.81081\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.3907 - accuracy: 0.8238 - val_loss: 0.4533 - val_accuracy: 0.7703\n",
            "Epoch 54/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.3705 - accuracy: 0.8370\n",
            "Epoch 54: val_accuracy did not improve from 0.81081\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.3683 - accuracy: 0.8384 - val_loss: 0.4238 - val_accuracy: 0.7838\n",
            "Epoch 55/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.3659 - accuracy: 0.8366\n",
            "Epoch 55: val_accuracy did not improve from 0.81081\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.3649 - accuracy: 0.8399 - val_loss: 0.4197 - val_accuracy: 0.7883\n",
            "Epoch 56/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.3805 - accuracy: 0.8237\n",
            "Epoch 56: val_accuracy did not improve from 0.81081\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.3866 - accuracy: 0.8198 - val_loss: 0.4344 - val_accuracy: 0.7658\n",
            "Epoch 57/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3784 - accuracy: 0.8288\n",
            "Epoch 57: val_accuracy did not improve from 0.81081\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.3784 - accuracy: 0.8288 - val_loss: 0.4672 - val_accuracy: 0.7748\n",
            "Epoch 58/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.4152 - accuracy: 0.8023\n",
            "Epoch 58: val_accuracy did not improve from 0.81081\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.4126 - accuracy: 0.8052 - val_loss: 0.4464 - val_accuracy: 0.7793\n",
            "Epoch 59/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.3955 - accuracy: 0.8041\n",
            "Epoch 59: val_accuracy did not improve from 0.81081\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.4017 - accuracy: 0.8017 - val_loss: 0.4710 - val_accuracy: 0.7838\n",
            "Epoch 60/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.3795 - accuracy: 0.8269\n",
            "Epoch 60: val_accuracy did not improve from 0.81081\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.3744 - accuracy: 0.8333 - val_loss: 0.4194 - val_accuracy: 0.7883\n",
            "Epoch 61/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.3689 - accuracy: 0.8238\n",
            "Epoch 61: val_accuracy did not improve from 0.81081\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.3691 - accuracy: 0.8258 - val_loss: 0.4424 - val_accuracy: 0.7838\n",
            "Epoch 62/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.3682 - accuracy: 0.8229\n",
            "Epoch 62: val_accuracy did not improve from 0.81081\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.3655 - accuracy: 0.8253 - val_loss: 0.3990 - val_accuracy: 0.7973\n",
            "Epoch 63/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.3485 - accuracy: 0.8492\n",
            "Epoch 63: val_accuracy did not improve from 0.81081\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.3420 - accuracy: 0.8544 - val_loss: 0.4378 - val_accuracy: 0.7838\n",
            "Epoch 64/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.3503 - accuracy: 0.8493\n",
            "Epoch 64: val_accuracy improved from 0.81081 to 0.81532, saving model to model.h5\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.3518 - accuracy: 0.8484 - val_loss: 0.4080 - val_accuracy: 0.8153\n",
            "Epoch 65/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.3486 - accuracy: 0.8504\n",
            "Epoch 65: val_accuracy did not improve from 0.81532\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.3481 - accuracy: 0.8484 - val_loss: 0.4022 - val_accuracy: 0.8018\n",
            "Epoch 66/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.3605 - accuracy: 0.8376\n",
            "Epoch 66: val_accuracy did not improve from 0.81532\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.3632 - accuracy: 0.8353 - val_loss: 0.4428 - val_accuracy: 0.7883\n",
            "Epoch 67/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.3424 - accuracy: 0.8365\n",
            "Epoch 67: val_accuracy improved from 0.81532 to 0.81982, saving model to model.h5\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.3362 - accuracy: 0.8394 - val_loss: 0.4022 - val_accuracy: 0.8198\n",
            "Epoch 68/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.3523 - accuracy: 0.8453\n",
            "Epoch 68: val_accuracy did not improve from 0.81982\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3463 - accuracy: 0.8439 - val_loss: 0.4509 - val_accuracy: 0.7883\n",
            "Epoch 69/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3407 - accuracy: 0.8514\n",
            "Epoch 69: val_accuracy did not improve from 0.81982\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.3407 - accuracy: 0.8514 - val_loss: 0.3964 - val_accuracy: 0.8018\n",
            "Epoch 70/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3207 - accuracy: 0.8630\n",
            "Epoch 70: val_accuracy did not improve from 0.81982\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.3207 - accuracy: 0.8630 - val_loss: 0.5428 - val_accuracy: 0.7523\n",
            "Epoch 71/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.3798 - accuracy: 0.8253\n",
            "Epoch 71: val_accuracy did not improve from 0.81982\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.3826 - accuracy: 0.8218 - val_loss: 0.4812 - val_accuracy: 0.7613\n",
            "Epoch 72/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.3254 - accuracy: 0.8547\n",
            "Epoch 72: val_accuracy did not improve from 0.81982\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.3309 - accuracy: 0.8529 - val_loss: 0.4024 - val_accuracy: 0.7883\n",
            "Epoch 73/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.3303 - accuracy: 0.8492\n",
            "Epoch 73: val_accuracy did not improve from 0.81982\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.3267 - accuracy: 0.8499 - val_loss: 0.4203 - val_accuracy: 0.7883\n",
            "Epoch 74/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.3226 - accuracy: 0.8582\n",
            "Epoch 74: val_accuracy did not improve from 0.81982\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.3223 - accuracy: 0.8584 - val_loss: 0.4259 - val_accuracy: 0.7973\n",
            "Epoch 75/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3049 - accuracy: 0.8645\n",
            "Epoch 75: val_accuracy did not improve from 0.81982\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.3049 - accuracy: 0.8645 - val_loss: 0.4003 - val_accuracy: 0.8108\n",
            "Epoch 76/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.3378 - accuracy: 0.8443\n",
            "Epoch 76: val_accuracy did not improve from 0.81982\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.3406 - accuracy: 0.8424 - val_loss: 0.4300 - val_accuracy: 0.8018\n",
            "Epoch 77/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.3138 - accuracy: 0.8594\n",
            "Epoch 77: val_accuracy improved from 0.81982 to 0.82883, saving model to model.h5\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.3157 - accuracy: 0.8614 - val_loss: 0.3634 - val_accuracy: 0.8288\n",
            "Epoch 78/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.3137 - accuracy: 0.8599\n",
            "Epoch 78: val_accuracy did not improve from 0.82883\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.3114 - accuracy: 0.8609 - val_loss: 0.4479 - val_accuracy: 0.7748\n",
            "Epoch 79/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.3043 - accuracy: 0.8717\n",
            "Epoch 79: val_accuracy did not improve from 0.82883\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.3021 - accuracy: 0.8720 - val_loss: 0.4024 - val_accuracy: 0.7928\n",
            "Epoch 80/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.2886 - accuracy: 0.8760\n",
            "Epoch 80: val_accuracy did not improve from 0.82883\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.2884 - accuracy: 0.8755 - val_loss: 0.4589 - val_accuracy: 0.8018\n",
            "Epoch 81/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.3075 - accuracy: 0.8529\n",
            "Epoch 81: val_accuracy did not improve from 0.82883\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.3070 - accuracy: 0.8564 - val_loss: 0.4017 - val_accuracy: 0.8198\n",
            "Epoch 82/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.3007 - accuracy: 0.8736\n",
            "Epoch 82: val_accuracy did not improve from 0.82883\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3085 - accuracy: 0.8685 - val_loss: 0.4488 - val_accuracy: 0.7793\n",
            "Epoch 83/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.3210 - accuracy: 0.8555\n",
            "Epoch 83: val_accuracy improved from 0.82883 to 0.84234, saving model to model.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.3284 - accuracy: 0.8534 - val_loss: 0.3627 - val_accuracy: 0.8423\n",
            "Epoch 84/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.3080 - accuracy: 0.8633\n",
            "Epoch 84: val_accuracy did not improve from 0.84234\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2978 - accuracy: 0.8700 - val_loss: 0.3570 - val_accuracy: 0.8423\n",
            "Epoch 85/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.2787 - accuracy: 0.8810\n",
            "Epoch 85: val_accuracy did not improve from 0.84234\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2726 - accuracy: 0.8835 - val_loss: 0.4296 - val_accuracy: 0.8108\n",
            "Epoch 86/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.2699 - accuracy: 0.8858\n",
            "Epoch 86: val_accuracy did not improve from 0.84234\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2642 - accuracy: 0.8881 - val_loss: 0.3819 - val_accuracy: 0.8288\n",
            "Epoch 87/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2681 - accuracy: 0.8855\n",
            "Epoch 87: val_accuracy did not improve from 0.84234\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.2681 - accuracy: 0.8855 - val_loss: 0.3736 - val_accuracy: 0.8198\n",
            "Epoch 88/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.2770 - accuracy: 0.8750\n",
            "Epoch 88: val_accuracy did not improve from 0.84234\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2810 - accuracy: 0.8755 - val_loss: 0.4022 - val_accuracy: 0.8108\n",
            "Epoch 89/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.2738 - accuracy: 0.8828\n",
            "Epoch 89: val_accuracy did not improve from 0.84234\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.2714 - accuracy: 0.8860 - val_loss: 0.3664 - val_accuracy: 0.8243\n",
            "Epoch 90/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.2551 - accuracy: 0.8994\n",
            "Epoch 90: val_accuracy improved from 0.84234 to 0.85135, saving model to model.h5\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.2627 - accuracy: 0.8921 - val_loss: 0.3546 - val_accuracy: 0.8514\n",
            "Epoch 91/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.2612 - accuracy: 0.8845\n",
            "Epoch 91: val_accuracy did not improve from 0.85135\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.2659 - accuracy: 0.8876 - val_loss: 0.3817 - val_accuracy: 0.8288\n",
            "Epoch 92/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.2727 - accuracy: 0.8819\n",
            "Epoch 92: val_accuracy did not improve from 0.85135\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.2611 - accuracy: 0.8870 - val_loss: 0.3719 - val_accuracy: 0.8018\n",
            "Epoch 93/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.3220 - accuracy: 0.8403\n",
            "Epoch 93: val_accuracy did not improve from 0.85135\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3025 - accuracy: 0.8549 - val_loss: 0.3799 - val_accuracy: 0.8333\n",
            "Epoch 94/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2736 - accuracy: 0.8805\n",
            "Epoch 94: val_accuracy did not improve from 0.85135\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.2736 - accuracy: 0.8805 - val_loss: 0.3829 - val_accuracy: 0.8108\n",
            "Epoch 95/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.2872 - accuracy: 0.8695\n",
            "Epoch 95: val_accuracy did not improve from 0.85135\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3107 - accuracy: 0.8559 - val_loss: 0.4657 - val_accuracy: 0.7928\n",
            "Epoch 96/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.3463 - accuracy: 0.8555\n",
            "Epoch 96: val_accuracy did not improve from 0.85135\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3347 - accuracy: 0.8549 - val_loss: 0.4525 - val_accuracy: 0.8153\n",
            "Epoch 97/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.2973 - accuracy: 0.8689\n",
            "Epoch 97: val_accuracy did not improve from 0.85135\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.2830 - accuracy: 0.8815 - val_loss: 0.4272 - val_accuracy: 0.8288\n",
            "Epoch 98/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.2749 - accuracy: 0.8828\n",
            "Epoch 98: val_accuracy improved from 0.85135 to 0.86486, saving model to model.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.2647 - accuracy: 0.8860 - val_loss: 0.3650 - val_accuracy: 0.8649\n",
            "Epoch 99/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.2658 - accuracy: 0.8912\n",
            "Epoch 99: val_accuracy did not improve from 0.86486\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.2685 - accuracy: 0.8886 - val_loss: 0.3747 - val_accuracy: 0.8423\n",
            "Epoch 100/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.2495 - accuracy: 0.8973\n",
            "Epoch 100: val_accuracy improved from 0.86486 to 0.86937, saving model to model.h5\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.2517 - accuracy: 0.8966 - val_loss: 0.3722 - val_accuracy: 0.8694\n",
            "Epoch 101/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2335 - accuracy: 0.9026\n",
            "Epoch 101: val_accuracy improved from 0.86937 to 0.87387, saving model to model.h5\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.2335 - accuracy: 0.9026 - val_loss: 0.3935 - val_accuracy: 0.8739\n",
            "Epoch 102/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.2311 - accuracy: 0.8898\n",
            "Epoch 102: val_accuracy did not improve from 0.87387\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2341 - accuracy: 0.8896 - val_loss: 0.3804 - val_accuracy: 0.8288\n",
            "Epoch 103/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.2636 - accuracy: 0.8906\n",
            "Epoch 103: val_accuracy did not improve from 0.87387\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2529 - accuracy: 0.8906 - val_loss: 0.4171 - val_accuracy: 0.8243\n",
            "Epoch 104/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.2399 - accuracy: 0.8976\n",
            "Epoch 104: val_accuracy did not improve from 0.87387\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.2530 - accuracy: 0.8906 - val_loss: 0.4860 - val_accuracy: 0.8108\n",
            "Epoch 105/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.2631 - accuracy: 0.8965\n",
            "Epoch 105: val_accuracy did not improve from 0.87387\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.2714 - accuracy: 0.8855 - val_loss: 0.3987 - val_accuracy: 0.8423\n",
            "Epoch 106/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.2404 - accuracy: 0.9062\n",
            "Epoch 106: val_accuracy did not improve from 0.87387\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2368 - accuracy: 0.9016 - val_loss: 0.3664 - val_accuracy: 0.8468\n",
            "Epoch 107/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.2232 - accuracy: 0.9091\n",
            "Epoch 107: val_accuracy did not improve from 0.87387\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2244 - accuracy: 0.9091 - val_loss: 0.3363 - val_accuracy: 0.8604\n",
            "Epoch 108/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.2062 - accuracy: 0.9099\n",
            "Epoch 108: val_accuracy did not improve from 0.87387\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2145 - accuracy: 0.9061 - val_loss: 0.5271 - val_accuracy: 0.8108\n",
            "Epoch 109/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.2391 - accuracy: 0.9055\n",
            "Epoch 109: val_accuracy did not improve from 0.87387\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2291 - accuracy: 0.9091 - val_loss: 0.3965 - val_accuracy: 0.8423\n",
            "Epoch 110/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.2130 - accuracy: 0.9201\n",
            "Epoch 110: val_accuracy did not improve from 0.87387\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2188 - accuracy: 0.9152 - val_loss: 0.4131 - val_accuracy: 0.8694\n",
            "Epoch 111/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.2341 - accuracy: 0.8970\n",
            "Epoch 111: val_accuracy did not improve from 0.87387\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2345 - accuracy: 0.8976 - val_loss: 0.3674 - val_accuracy: 0.8514\n",
            "Epoch 112/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.2164 - accuracy: 0.9023\n",
            "Epoch 112: val_accuracy did not improve from 0.87387\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2120 - accuracy: 0.9036 - val_loss: 0.3389 - val_accuracy: 0.8604\n",
            "Epoch 113/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.2107 - accuracy: 0.9148\n",
            "Epoch 113: val_accuracy did not improve from 0.87387\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.2038 - accuracy: 0.9177 - val_loss: 0.3694 - val_accuracy: 0.8559\n",
            "Epoch 114/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.2152 - accuracy: 0.9099\n",
            "Epoch 114: val_accuracy did not improve from 0.87387\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2095 - accuracy: 0.9137 - val_loss: 0.4716 - val_accuracy: 0.7973\n",
            "Epoch 115/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.2170 - accuracy: 0.9056\n",
            "Epoch 115: val_accuracy improved from 0.87387 to 0.88288, saving model to model.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.2244 - accuracy: 0.9001 - val_loss: 0.3422 - val_accuracy: 0.8829\n",
            "Epoch 116/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.2479 - accuracy: 0.8893\n",
            "Epoch 116: val_accuracy did not improve from 0.88288\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2399 - accuracy: 0.8956 - val_loss: 0.3658 - val_accuracy: 0.8829\n",
            "Epoch 117/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.2407 - accuracy: 0.8906\n",
            "Epoch 117: val_accuracy did not improve from 0.88288\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2366 - accuracy: 0.8921 - val_loss: 0.4208 - val_accuracy: 0.8514\n",
            "Epoch 118/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.2321 - accuracy: 0.8996\n",
            "Epoch 118: val_accuracy improved from 0.88288 to 0.88739, saving model to model.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2358 - accuracy: 0.8981 - val_loss: 0.3728 - val_accuracy: 0.8874\n",
            "Epoch 119/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.2453 - accuracy: 0.8956\n",
            "Epoch 119: val_accuracy did not improve from 0.88739\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2387 - accuracy: 0.8981 - val_loss: 0.3795 - val_accuracy: 0.8559\n",
            "Epoch 120/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.2068 - accuracy: 0.9069\n",
            "Epoch 120: val_accuracy did not improve from 0.88739\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2053 - accuracy: 0.9101 - val_loss: 0.3732 - val_accuracy: 0.8694\n",
            "Epoch 121/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.1780 - accuracy: 0.9245\n",
            "Epoch 121: val_accuracy did not improve from 0.88739\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1784 - accuracy: 0.9227 - val_loss: 0.4417 - val_accuracy: 0.8288\n",
            "Epoch 122/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.2026 - accuracy: 0.9102\n",
            "Epoch 122: val_accuracy did not improve from 0.88739\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2005 - accuracy: 0.9116 - val_loss: 0.3544 - val_accuracy: 0.8739\n",
            "Epoch 123/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.1726 - accuracy: 0.9249\n",
            "Epoch 123: val_accuracy improved from 0.88739 to 0.89640, saving model to model.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1764 - accuracy: 0.9207 - val_loss: 0.3429 - val_accuracy: 0.8964\n",
            "Epoch 124/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.1956 - accuracy: 0.9219\n",
            "Epoch 124: val_accuracy did not improve from 0.89640\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1996 - accuracy: 0.9172 - val_loss: 0.3390 - val_accuracy: 0.8829\n",
            "Epoch 125/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.1590 - accuracy: 0.9430\n",
            "Epoch 125: val_accuracy improved from 0.89640 to 0.90991, saving model to model.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1739 - accuracy: 0.9312 - val_loss: 0.3427 - val_accuracy: 0.9099\n",
            "Epoch 126/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.1668 - accuracy: 0.9375\n",
            "Epoch 126: val_accuracy did not improve from 0.90991\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1704 - accuracy: 0.9352 - val_loss: 0.3423 - val_accuracy: 0.8874\n",
            "Epoch 127/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.1874 - accuracy: 0.9212\n",
            "Epoch 127: val_accuracy did not improve from 0.90991\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1795 - accuracy: 0.9267 - val_loss: 0.3341 - val_accuracy: 0.9009\n",
            "Epoch 128/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.1659 - accuracy: 0.9321\n",
            "Epoch 128: val_accuracy did not improve from 0.90991\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1666 - accuracy: 0.9292 - val_loss: 0.3234 - val_accuracy: 0.9009\n",
            "Epoch 129/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.1498 - accuracy: 0.9405\n",
            "Epoch 129: val_accuracy did not improve from 0.90991\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1507 - accuracy: 0.9378 - val_loss: 0.3734 - val_accuracy: 0.8919\n",
            "Epoch 130/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.1593 - accuracy: 0.9342\n",
            "Epoch 130: val_accuracy did not improve from 0.90991\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1566 - accuracy: 0.9378 - val_loss: 0.3677 - val_accuracy: 0.8919\n",
            "Epoch 131/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.1378 - accuracy: 0.9564\n",
            "Epoch 131: val_accuracy did not improve from 0.90991\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1472 - accuracy: 0.9503 - val_loss: 0.3671 - val_accuracy: 0.8649\n",
            "Epoch 132/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.1807 - accuracy: 0.9279\n",
            "Epoch 132: val_accuracy did not improve from 0.90991\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1768 - accuracy: 0.9282 - val_loss: 0.3607 - val_accuracy: 0.8919\n",
            "Epoch 133/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.1598 - accuracy: 0.9361\n",
            "Epoch 133: val_accuracy did not improve from 0.90991\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1552 - accuracy: 0.9362 - val_loss: 0.3646 - val_accuracy: 0.8964\n",
            "Epoch 134/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.1508 - accuracy: 0.9453\n",
            "Epoch 134: val_accuracy did not improve from 0.90991\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1491 - accuracy: 0.9453 - val_loss: 0.3636 - val_accuracy: 0.9054\n",
            "Epoch 135/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.1424 - accuracy: 0.9497\n",
            "Epoch 135: val_accuracy did not improve from 0.90991\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1559 - accuracy: 0.9433 - val_loss: 0.3434 - val_accuracy: 0.9054\n",
            "Epoch 136/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.1460 - accuracy: 0.9411\n",
            "Epoch 136: val_accuracy improved from 0.90991 to 0.91892, saving model to model.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1395 - accuracy: 0.9488 - val_loss: 0.3155 - val_accuracy: 0.9189\n",
            "Epoch 137/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.1301 - accuracy: 0.9518\n",
            "Epoch 137: val_accuracy did not improve from 0.91892\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1275 - accuracy: 0.9528 - val_loss: 0.3414 - val_accuracy: 0.9189\n",
            "Epoch 138/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.1372 - accuracy: 0.9486\n",
            "Epoch 138: val_accuracy did not improve from 0.91892\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1417 - accuracy: 0.9448 - val_loss: 0.3523 - val_accuracy: 0.9054\n",
            "Epoch 139/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.1446 - accuracy: 0.9362\n",
            "Epoch 139: val_accuracy did not improve from 0.91892\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1542 - accuracy: 0.9342 - val_loss: 0.3808 - val_accuracy: 0.8559\n",
            "Epoch 140/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.1754 - accuracy: 0.9219\n",
            "Epoch 140: val_accuracy did not improve from 0.91892\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1864 - accuracy: 0.9172 - val_loss: 0.3963 - val_accuracy: 0.8559\n",
            "Epoch 141/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.2062 - accuracy: 0.9013\n",
            "Epoch 141: val_accuracy did not improve from 0.91892\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2106 - accuracy: 0.9021 - val_loss: 0.3392 - val_accuracy: 0.9189\n",
            "Epoch 142/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.1760 - accuracy: 0.9219\n",
            "Epoch 142: val_accuracy did not improve from 0.91892\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1842 - accuracy: 0.9187 - val_loss: 0.3963 - val_accuracy: 0.8559\n",
            "Epoch 143/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.2382 - accuracy: 0.8942\n",
            "Epoch 143: val_accuracy did not improve from 0.91892\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2295 - accuracy: 0.9001 - val_loss: 0.4690 - val_accuracy: 0.8514\n",
            "Epoch 144/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.2114 - accuracy: 0.9098\n",
            "Epoch 144: val_accuracy did not improve from 0.91892\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.2008 - accuracy: 0.9121 - val_loss: 0.3685 - val_accuracy: 0.8784\n",
            "Epoch 145/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.1633 - accuracy: 0.9289\n",
            "Epoch 145: val_accuracy did not improve from 0.91892\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1621 - accuracy: 0.9317 - val_loss: 0.3750 - val_accuracy: 0.9144\n",
            "Epoch 146/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.1337 - accuracy: 0.9503\n",
            "Epoch 146: val_accuracy did not improve from 0.91892\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1314 - accuracy: 0.9488 - val_loss: 0.3104 - val_accuracy: 0.9144\n",
            "Epoch 147/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.1216 - accuracy: 0.9583\n",
            "Epoch 147: val_accuracy did not improve from 0.91892\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1234 - accuracy: 0.9563 - val_loss: 0.3274 - val_accuracy: 0.9144\n",
            "Epoch 148/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.1465 - accuracy: 0.9445\n",
            "Epoch 148: val_accuracy did not improve from 0.91892\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1474 - accuracy: 0.9413 - val_loss: 0.3471 - val_accuracy: 0.8919\n",
            "Epoch 149/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.1443 - accuracy: 0.9473\n",
            "Epoch 149: val_accuracy did not improve from 0.91892\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1389 - accuracy: 0.9513 - val_loss: 0.3457 - val_accuracy: 0.9054\n",
            "Epoch 150/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.1284 - accuracy: 0.9514\n",
            "Epoch 150: val_accuracy did not improve from 0.91892\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1249 - accuracy: 0.9518 - val_loss: 0.3778 - val_accuracy: 0.9099\n",
            "Epoch 151/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.1284 - accuracy: 0.9561\n",
            "Epoch 151: val_accuracy did not improve from 0.91892\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1254 - accuracy: 0.9568 - val_loss: 0.3248 - val_accuracy: 0.9099\n",
            "Epoch 152/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.1151 - accuracy: 0.9560\n",
            "Epoch 152: val_accuracy did not improve from 0.91892\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1225 - accuracy: 0.9528 - val_loss: 0.3480 - val_accuracy: 0.9009\n",
            "Epoch 153/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.1045 - accuracy: 0.9622\n",
            "Epoch 153: val_accuracy did not improve from 0.91892\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1058 - accuracy: 0.9603 - val_loss: 0.4484 - val_accuracy: 0.8649\n",
            "Epoch 154/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.1496 - accuracy: 0.9466\n",
            "Epoch 154: val_accuracy did not improve from 0.91892\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1429 - accuracy: 0.9493 - val_loss: 0.3374 - val_accuracy: 0.9054\n",
            "Epoch 155/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.1137 - accuracy: 0.9574\n",
            "Epoch 155: val_accuracy did not improve from 0.91892\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1243 - accuracy: 0.9508 - val_loss: 0.3572 - val_accuracy: 0.9099\n",
            "Epoch 156/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.1068 - accuracy: 0.9579\n",
            "Epoch 156: val_accuracy improved from 0.91892 to 0.92342, saving model to model.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1059 - accuracy: 0.9578 - val_loss: 0.3240 - val_accuracy: 0.9234\n",
            "Epoch 157/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.1041 - accuracy: 0.9624\n",
            "Epoch 157: val_accuracy did not improve from 0.92342\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1074 - accuracy: 0.9603 - val_loss: 0.3843 - val_accuracy: 0.8964\n",
            "Epoch 158/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.1027 - accuracy: 0.9645\n",
            "Epoch 158: val_accuracy did not improve from 0.92342\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1026 - accuracy: 0.9639 - val_loss: 0.2921 - val_accuracy: 0.9234\n",
            "Epoch 159/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.1035 - accuracy: 0.9557\n",
            "Epoch 159: val_accuracy did not improve from 0.92342\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1028 - accuracy: 0.9578 - val_loss: 0.3098 - val_accuracy: 0.9189\n",
            "Epoch 160/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.1014 - accuracy: 0.9639\n",
            "Epoch 160: val_accuracy improved from 0.92342 to 0.93694, saving model to model.h5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0989 - accuracy: 0.9649 - val_loss: 0.3175 - val_accuracy: 0.9369\n",
            "Epoch 161/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.1215 - accuracy: 0.9543\n",
            "Epoch 161: val_accuracy did not improve from 0.93694\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1162 - accuracy: 0.9568 - val_loss: 0.3887 - val_accuracy: 0.9099\n",
            "Epoch 162/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0998 - accuracy: 0.9648\n",
            "Epoch 162: val_accuracy did not improve from 0.93694\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1019 - accuracy: 0.9608 - val_loss: 0.3476 - val_accuracy: 0.9369\n",
            "Epoch 163/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0894 - accuracy: 0.9659\n",
            "Epoch 163: val_accuracy did not improve from 0.93694\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0913 - accuracy: 0.9654 - val_loss: 0.3406 - val_accuracy: 0.9324\n",
            "Epoch 164/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0932 - accuracy: 0.9737\n",
            "Epoch 164: val_accuracy did not improve from 0.93694\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1002 - accuracy: 0.9689 - val_loss: 0.3470 - val_accuracy: 0.9324\n",
            "Epoch 165/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.1004 - accuracy: 0.9602\n",
            "Epoch 165: val_accuracy did not improve from 0.93694\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1155 - accuracy: 0.9453 - val_loss: 0.3992 - val_accuracy: 0.9099\n",
            "Epoch 166/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.1300 - accuracy: 0.9414\n",
            "Epoch 166: val_accuracy did not improve from 0.93694\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1418 - accuracy: 0.9372 - val_loss: 0.3703 - val_accuracy: 0.8964\n",
            "Epoch 167/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.1298 - accuracy: 0.9523\n",
            "Epoch 167: val_accuracy did not improve from 0.93694\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1292 - accuracy: 0.9533 - val_loss: 0.3121 - val_accuracy: 0.9189\n",
            "Epoch 168/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.1059 - accuracy: 0.9595\n",
            "Epoch 168: val_accuracy did not improve from 0.93694\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1120 - accuracy: 0.9588 - val_loss: 0.2985 - val_accuracy: 0.9324\n",
            "Epoch 169/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.1031 - accuracy: 0.9629\n",
            "Epoch 169: val_accuracy did not improve from 0.93694\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1086 - accuracy: 0.9593 - val_loss: 0.3107 - val_accuracy: 0.9279\n",
            "Epoch 170/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0882 - accuracy: 0.9688\n",
            "Epoch 170: val_accuracy improved from 0.93694 to 0.94595, saving model to model.h5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0921 - accuracy: 0.9669 - val_loss: 0.2911 - val_accuracy: 0.9459\n",
            "Epoch 171/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0959 - accuracy: 0.9651\n",
            "Epoch 171: val_accuracy did not improve from 0.94595\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0950 - accuracy: 0.9654 - val_loss: 0.3475 - val_accuracy: 0.9279\n",
            "Epoch 172/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0885 - accuracy: 0.9722\n",
            "Epoch 172: val_accuracy did not improve from 0.94595\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0948 - accuracy: 0.9674 - val_loss: 0.3286 - val_accuracy: 0.9459\n",
            "Epoch 173/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0916 - accuracy: 0.9695\n",
            "Epoch 173: val_accuracy did not improve from 0.94595\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0950 - accuracy: 0.9669 - val_loss: 0.3458 - val_accuracy: 0.9144\n",
            "Epoch 174/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0788 - accuracy: 0.9772\n",
            "Epoch 174: val_accuracy did not improve from 0.94595\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0786 - accuracy: 0.9774 - val_loss: 0.3445 - val_accuracy: 0.9414\n",
            "Epoch 175/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0698 - accuracy: 0.9740\n",
            "Epoch 175: val_accuracy did not improve from 0.94595\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0736 - accuracy: 0.9739 - val_loss: 0.3406 - val_accuracy: 0.9279\n",
            "Epoch 176/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0778 - accuracy: 0.9737\n",
            "Epoch 176: val_accuracy did not improve from 0.94595\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0813 - accuracy: 0.9709 - val_loss: 0.3655 - val_accuracy: 0.9144\n",
            "Epoch 177/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0776 - accuracy: 0.9737\n",
            "Epoch 177: val_accuracy did not improve from 0.94595\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0849 - accuracy: 0.9699 - val_loss: 0.3703 - val_accuracy: 0.9234\n",
            "Epoch 178/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0884 - accuracy: 0.9684\n",
            "Epoch 178: val_accuracy did not improve from 0.94595\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0884 - accuracy: 0.9684 - val_loss: 0.3198 - val_accuracy: 0.9414\n",
            "Epoch 179/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0795 - accuracy: 0.9749\n",
            "Epoch 179: val_accuracy did not improve from 0.94595\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0785 - accuracy: 0.9754 - val_loss: 0.3450 - val_accuracy: 0.9189\n",
            "Epoch 180/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0671 - accuracy: 0.9792\n",
            "Epoch 180: val_accuracy improved from 0.94595 to 0.95045, saving model to model.h5\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0728 - accuracy: 0.9744 - val_loss: 0.3003 - val_accuracy: 0.9505\n",
            "Epoch 181/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9699\n",
            "Epoch 181: val_accuracy improved from 0.95045 to 0.95495, saving model to model.h5\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0784 - accuracy: 0.9699 - val_loss: 0.2972 - val_accuracy: 0.9550\n",
            "Epoch 182/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.1065 - accuracy: 0.9542\n",
            "Epoch 182: val_accuracy did not improve from 0.95495\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1041 - accuracy: 0.9558 - val_loss: 0.4824 - val_accuracy: 0.8649\n",
            "Epoch 183/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.1411 - accuracy: 0.9453\n",
            "Epoch 183: val_accuracy did not improve from 0.95495\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.1371 - accuracy: 0.9473 - val_loss: 0.3626 - val_accuracy: 0.9234\n",
            "Epoch 184/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0797 - accuracy: 0.9648\n",
            "Epoch 184: val_accuracy did not improve from 0.95495\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0781 - accuracy: 0.9649 - val_loss: 0.3188 - val_accuracy: 0.9414\n",
            "Epoch 185/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9754\n",
            "Epoch 185: val_accuracy did not improve from 0.95495\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0707 - accuracy: 0.9754 - val_loss: 0.3122 - val_accuracy: 0.9414\n",
            "Epoch 186/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0733 - accuracy: 0.9748\n",
            "Epoch 186: val_accuracy did not improve from 0.95495\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0642 - accuracy: 0.9804 - val_loss: 0.3108 - val_accuracy: 0.9414\n",
            "Epoch 187/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0540 - accuracy: 0.9844\n",
            "Epoch 187: val_accuracy improved from 0.95495 to 0.95946, saving model to model.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0501 - accuracy: 0.9854 - val_loss: 0.2873 - val_accuracy: 0.9595\n",
            "Epoch 188/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0495 - accuracy: 0.9828\n",
            "Epoch 188: val_accuracy did not improve from 0.95946\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0671 - accuracy: 0.9789 - val_loss: 0.3190 - val_accuracy: 0.9459\n",
            "Epoch 189/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0684 - accuracy: 0.9792\n",
            "Epoch 189: val_accuracy did not improve from 0.95946\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0675 - accuracy: 0.9794 - val_loss: 0.2909 - val_accuracy: 0.9505\n",
            "Epoch 190/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0599 - accuracy: 0.9794\n",
            "Epoch 190: val_accuracy did not improve from 0.95946\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0617 - accuracy: 0.9779 - val_loss: 0.3200 - val_accuracy: 0.9459\n",
            "Epoch 191/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0601 - accuracy: 0.9757\n",
            "Epoch 191: val_accuracy did not improve from 0.95946\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0566 - accuracy: 0.9784 - val_loss: 0.3664 - val_accuracy: 0.9234\n",
            "Epoch 192/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0612 - accuracy: 0.9814\n",
            "Epoch 192: val_accuracy did not improve from 0.95946\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0611 - accuracy: 0.9809 - val_loss: 0.3392 - val_accuracy: 0.9369\n",
            "Epoch 193/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0559 - accuracy: 0.9812\n",
            "Epoch 193: val_accuracy did not improve from 0.95946\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0509 - accuracy: 0.9824 - val_loss: 0.2961 - val_accuracy: 0.9505\n",
            "Epoch 194/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0480 - accuracy: 0.9836\n",
            "Epoch 194: val_accuracy improved from 0.95946 to 0.97748, saving model to model.h5\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0458 - accuracy: 0.9854 - val_loss: 0.2981 - val_accuracy: 0.9775\n",
            "Epoch 195/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0403 - accuracy: 0.9878\n",
            "Epoch 195: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0362 - accuracy: 0.9885 - val_loss: 0.3079 - val_accuracy: 0.9550\n",
            "Epoch 196/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0508 - accuracy: 0.9812\n",
            "Epoch 196: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0495 - accuracy: 0.9834 - val_loss: 0.3046 - val_accuracy: 0.9459\n",
            "Epoch 197/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0304 - accuracy: 0.9915\n",
            "Epoch 197: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0300 - accuracy: 0.9905 - val_loss: 0.2862 - val_accuracy: 0.9685\n",
            "Epoch 198/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0327 - accuracy: 0.9908\n",
            "Epoch 198: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0375 - accuracy: 0.9890 - val_loss: 0.3242 - val_accuracy: 0.9550\n",
            "Epoch 199/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0392 - accuracy: 0.9844\n",
            "Epoch 199: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0389 - accuracy: 0.9844 - val_loss: 0.3369 - val_accuracy: 0.9550\n",
            "Epoch 200/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0646 - accuracy: 0.9759\n",
            "Epoch 200: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0665 - accuracy: 0.9759 - val_loss: 0.3524 - val_accuracy: 0.9505\n",
            "Epoch 201/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0867 - accuracy: 0.9648\n",
            "Epoch 201: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1037 - accuracy: 0.9558 - val_loss: 0.3339 - val_accuracy: 0.9369\n",
            "Epoch 202/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.1413 - accuracy: 0.9460\n",
            "Epoch 202: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1465 - accuracy: 0.9488 - val_loss: 0.3912 - val_accuracy: 0.9279\n",
            "Epoch 203/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.1303 - accuracy: 0.9510\n",
            "Epoch 203: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1177 - accuracy: 0.9528 - val_loss: 0.3488 - val_accuracy: 0.9279\n",
            "Epoch 204/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.1059 - accuracy: 0.9616\n",
            "Epoch 204: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1086 - accuracy: 0.9593 - val_loss: 0.4442 - val_accuracy: 0.8919\n",
            "Epoch 205/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0897 - accuracy: 0.9680\n",
            "Epoch 205: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0813 - accuracy: 0.9709 - val_loss: 0.3376 - val_accuracy: 0.9324\n",
            "Epoch 206/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0718 - accuracy: 0.9688\n",
            "Epoch 206: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0683 - accuracy: 0.9699 - val_loss: 0.3388 - val_accuracy: 0.9459\n",
            "Epoch 207/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0687 - accuracy: 0.9838\n",
            "Epoch 207: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0625 - accuracy: 0.9859 - val_loss: 0.3050 - val_accuracy: 0.9459\n",
            "Epoch 208/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0567 - accuracy: 0.9865\n",
            "Epoch 208: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0571 - accuracy: 0.9844 - val_loss: 0.3505 - val_accuracy: 0.9414\n",
            "Epoch 209/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0754 - accuracy: 0.9746\n",
            "Epoch 209: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0778 - accuracy: 0.9749 - val_loss: 0.2798 - val_accuracy: 0.9505\n",
            "Epoch 210/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.1022 - accuracy: 0.9688\n",
            "Epoch 210: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0908 - accuracy: 0.9719 - val_loss: 0.3523 - val_accuracy: 0.9550\n",
            "Epoch 211/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0623 - accuracy: 0.9773\n",
            "Epoch 211: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0839 - accuracy: 0.9744 - val_loss: 0.3281 - val_accuracy: 0.9414\n",
            "Epoch 212/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.1767 - accuracy: 0.9332\n",
            "Epoch 212: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1846 - accuracy: 0.9347 - val_loss: 0.4002 - val_accuracy: 0.8739\n",
            "Epoch 213/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.1525 - accuracy: 0.9368\n",
            "Epoch 213: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1467 - accuracy: 0.9408 - val_loss: 0.3681 - val_accuracy: 0.9144\n",
            "Epoch 214/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.1332 - accuracy: 0.9479\n",
            "Epoch 214: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1409 - accuracy: 0.9468 - val_loss: 0.3388 - val_accuracy: 0.9279\n",
            "Epoch 215/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.1030 - accuracy: 0.9577\n",
            "Epoch 215: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0975 - accuracy: 0.9608 - val_loss: 0.2844 - val_accuracy: 0.9414\n",
            "Epoch 216/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0494 - accuracy: 0.9826\n",
            "Epoch 216: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0494 - accuracy: 0.9844 - val_loss: 0.2525 - val_accuracy: 0.9775\n",
            "Epoch 217/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0467 - accuracy: 0.9836\n",
            "Epoch 217: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0448 - accuracy: 0.9839 - val_loss: 0.3061 - val_accuracy: 0.9505\n",
            "Epoch 218/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0450 - accuracy: 0.9812\n",
            "Epoch 218: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0440 - accuracy: 0.9829 - val_loss: 0.2580 - val_accuracy: 0.9730\n",
            "Epoch 219/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0345 - accuracy: 0.9939\n",
            "Epoch 219: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0364 - accuracy: 0.9910 - val_loss: 0.3149 - val_accuracy: 0.9505\n",
            "Epoch 220/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0329 - accuracy: 0.9896\n",
            "Epoch 220: val_accuracy did not improve from 0.97748\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0321 - accuracy: 0.9885 - val_loss: 0.2362 - val_accuracy: 0.9730\n",
            "Epoch 221/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0269 - accuracy: 0.9950\n",
            "Epoch 221: val_accuracy improved from 0.97748 to 0.98198, saving model to model.h5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0254 - accuracy: 0.9955 - val_loss: 0.2772 - val_accuracy: 0.9820\n",
            "Epoch 222/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0213 - accuracy: 0.9972\n",
            "Epoch 222: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0217 - accuracy: 0.9975 - val_loss: 0.2407 - val_accuracy: 0.9820\n",
            "Epoch 223/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0200 - accuracy: 0.9950\n",
            "Epoch 223: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0229 - accuracy: 0.9950 - val_loss: 0.2771 - val_accuracy: 0.9685\n",
            "Epoch 224/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0277 - accuracy: 0.9928\n",
            "Epoch 224: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0280 - accuracy: 0.9925 - val_loss: 0.2509 - val_accuracy: 0.9730\n",
            "Epoch 225/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9905\n",
            "Epoch 225: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0285 - accuracy: 0.9905 - val_loss: 0.2658 - val_accuracy: 0.9730\n",
            "Epoch 226/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0274 - accuracy: 0.9936\n",
            "Epoch 226: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0258 - accuracy: 0.9940 - val_loss: 0.3105 - val_accuracy: 0.9640\n",
            "Epoch 227/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0291 - accuracy: 0.9935\n",
            "Epoch 227: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0288 - accuracy: 0.9935 - val_loss: 0.2936 - val_accuracy: 0.9775\n",
            "Epoch 228/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0257 - accuracy: 0.9941\n",
            "Epoch 228: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0300 - accuracy: 0.9915 - val_loss: 0.2374 - val_accuracy: 0.9820\n",
            "Epoch 229/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0336 - accuracy: 0.9904\n",
            "Epoch 229: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0336 - accuracy: 0.9905 - val_loss: 0.2854 - val_accuracy: 0.9730\n",
            "Epoch 230/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0384 - accuracy: 0.9850\n",
            "Epoch 230: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0370 - accuracy: 0.9859 - val_loss: 0.2696 - val_accuracy: 0.9685\n",
            "Epoch 231/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0263 - accuracy: 0.9936\n",
            "Epoch 231: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0319 - accuracy: 0.9920 - val_loss: 0.2991 - val_accuracy: 0.9685\n",
            "Epoch 232/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0619 - accuracy: 0.9759\n",
            "Epoch 232: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0767 - accuracy: 0.9684 - val_loss: 0.3227 - val_accuracy: 0.9505\n",
            "Epoch 233/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.1102 - accuracy: 0.9427\n",
            "Epoch 233: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0932 - accuracy: 0.9548 - val_loss: 0.2863 - val_accuracy: 0.9640\n",
            "Epoch 234/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0508 - accuracy: 0.9795\n",
            "Epoch 234: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0601 - accuracy: 0.9754 - val_loss: 0.3270 - val_accuracy: 0.9459\n",
            "Epoch 235/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0561 - accuracy: 0.9805\n",
            "Epoch 235: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0569 - accuracy: 0.9824 - val_loss: 0.2605 - val_accuracy: 0.9775\n",
            "Epoch 236/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0403 - accuracy: 0.9883\n",
            "Epoch 236: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0386 - accuracy: 0.9900 - val_loss: 0.3066 - val_accuracy: 0.9595\n",
            "Epoch 237/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0407 - accuracy: 0.9886\n",
            "Epoch 237: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0451 - accuracy: 0.9874 - val_loss: 0.2458 - val_accuracy: 0.9730\n",
            "Epoch 238/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0491 - accuracy: 0.9867\n",
            "Epoch 238: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0498 - accuracy: 0.9864 - val_loss: 0.3692 - val_accuracy: 0.9595\n",
            "Epoch 239/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0575 - accuracy: 0.9814\n",
            "Epoch 239: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0616 - accuracy: 0.9804 - val_loss: 0.3101 - val_accuracy: 0.9595\n",
            "Epoch 240/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0510 - accuracy: 0.9800\n",
            "Epoch 240: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0513 - accuracy: 0.9799 - val_loss: 0.2715 - val_accuracy: 0.9640\n",
            "Epoch 241/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0454 - accuracy: 0.9822\n",
            "Epoch 241: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0445 - accuracy: 0.9839 - val_loss: 0.2916 - val_accuracy: 0.9640\n",
            "Epoch 242/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0363 - accuracy: 0.9909\n",
            "Epoch 242: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0416 - accuracy: 0.9890 - val_loss: 0.3040 - val_accuracy: 0.9640\n",
            "Epoch 243/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0462 - accuracy: 0.9822\n",
            "Epoch 243: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0400 - accuracy: 0.9859 - val_loss: 0.2989 - val_accuracy: 0.9685\n",
            "Epoch 244/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0396 - accuracy: 0.9870\n",
            "Epoch 244: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0413 - accuracy: 0.9885 - val_loss: 0.3336 - val_accuracy: 0.9459\n",
            "Epoch 245/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0297 - accuracy: 0.9891\n",
            "Epoch 245: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0308 - accuracy: 0.9895 - val_loss: 0.3513 - val_accuracy: 0.9550\n",
            "Epoch 246/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0359 - accuracy: 0.9872\n",
            "Epoch 246: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0334 - accuracy: 0.9880 - val_loss: 0.2698 - val_accuracy: 0.9820\n",
            "Epoch 247/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0184 - accuracy: 0.9943\n",
            "Epoch 247: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0208 - accuracy: 0.9925 - val_loss: 0.2782 - val_accuracy: 0.9640\n",
            "Epoch 248/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0196 - accuracy: 0.9961\n",
            "Epoch 248: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0187 - accuracy: 0.9960 - val_loss: 0.2933 - val_accuracy: 0.9775\n",
            "Epoch 249/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0145 - accuracy: 0.9982\n",
            "Epoch 249: val_accuracy did not improve from 0.98198\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0141 - accuracy: 0.9985 - val_loss: 0.2728 - val_accuracy: 0.9775\n",
            "Epoch 250/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0094 - accuracy: 0.9993\n",
            "Epoch 250: val_accuracy improved from 0.98198 to 0.98649, saving model to model.h5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0106 - accuracy: 0.9985 - val_loss: 0.2638 - val_accuracy: 0.9865\n",
            "Epoch 251/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0102 - accuracy: 0.9979\n",
            "Epoch 251: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0101 - accuracy: 0.9985 - val_loss: 0.2776 - val_accuracy: 0.9730\n",
            "Epoch 252/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0106 - accuracy: 0.9977\n",
            "Epoch 252: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0119 - accuracy: 0.9975 - val_loss: 0.2725 - val_accuracy: 0.9820\n",
            "Epoch 253/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0140 - accuracy: 0.9961\n",
            "Epoch 253: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0131 - accuracy: 0.9965 - val_loss: 0.2926 - val_accuracy: 0.9730\n",
            "Epoch 254/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0115 - accuracy: 0.9991\n",
            "Epoch 254: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0137 - accuracy: 0.9975 - val_loss: 0.2792 - val_accuracy: 0.9730\n",
            "Epoch 255/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0430 - accuracy: 0.9913\n",
            "Epoch 255: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0528 - accuracy: 0.9864 - val_loss: 0.3061 - val_accuracy: 0.9640\n",
            "Epoch 256/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0958 - accuracy: 0.9629\n",
            "Epoch 256: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0991 - accuracy: 0.9634 - val_loss: 0.5818 - val_accuracy: 0.8559\n",
            "Epoch 257/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.1170 - accuracy: 0.9508\n",
            "Epoch 257: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1272 - accuracy: 0.9488 - val_loss: 0.3342 - val_accuracy: 0.9414\n",
            "Epoch 258/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.1433 - accuracy: 0.9473\n",
            "Epoch 258: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1548 - accuracy: 0.9408 - val_loss: 0.4461 - val_accuracy: 0.9009\n",
            "Epoch 259/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.1824 - accuracy: 0.9382\n",
            "Epoch 259: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1720 - accuracy: 0.9383 - val_loss: 0.5482 - val_accuracy: 0.8649\n",
            "Epoch 260/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.1493 - accuracy: 0.9471\n",
            "Epoch 260: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1457 - accuracy: 0.9478 - val_loss: 0.3643 - val_accuracy: 0.9279\n",
            "Epoch 261/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0784 - accuracy: 0.9792\n",
            "Epoch 261: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0776 - accuracy: 0.9759 - val_loss: 0.3445 - val_accuracy: 0.9414\n",
            "Epoch 262/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0656 - accuracy: 0.9775\n",
            "Epoch 262: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0553 - accuracy: 0.9824 - val_loss: 0.2955 - val_accuracy: 0.9459\n",
            "Epoch 263/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0387 - accuracy: 0.9893\n",
            "Epoch 263: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0392 - accuracy: 0.9900 - val_loss: 0.3339 - val_accuracy: 0.9459\n",
            "Epoch 264/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0488 - accuracy: 0.9857\n",
            "Epoch 264: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0491 - accuracy: 0.9849 - val_loss: 0.3052 - val_accuracy: 0.9685\n",
            "Epoch 265/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.1079 - accuracy: 0.9645\n",
            "Epoch 265: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1012 - accuracy: 0.9644 - val_loss: 0.2864 - val_accuracy: 0.9730\n",
            "Epoch 266/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0801 - accuracy: 0.9703\n",
            "Epoch 266: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0741 - accuracy: 0.9719 - val_loss: 0.3565 - val_accuracy: 0.9414\n",
            "Epoch 267/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9844\n",
            "Epoch 267: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0484 - accuracy: 0.9844 - val_loss: 0.2659 - val_accuracy: 0.9865\n",
            "Epoch 268/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0219 - accuracy: 0.9955\n",
            "Epoch 268: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0215 - accuracy: 0.9960 - val_loss: 0.2819 - val_accuracy: 0.9820\n",
            "Epoch 269/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0136 - accuracy: 0.9983\n",
            "Epoch 269: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0142 - accuracy: 0.9980 - val_loss: 0.2772 - val_accuracy: 0.9820\n",
            "Epoch 270/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0128 - accuracy: 0.9988\n",
            "Epoch 270: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0132 - accuracy: 0.9980 - val_loss: 0.3077 - val_accuracy: 0.9730\n",
            "Epoch 271/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9955\n",
            "Epoch 271: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0174 - accuracy: 0.9955 - val_loss: 0.3004 - val_accuracy: 0.9775\n",
            "Epoch 272/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0172 - accuracy: 0.9974\n",
            "Epoch 272: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0151 - accuracy: 0.9975 - val_loss: 0.2629 - val_accuracy: 0.9820\n",
            "Epoch 273/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0108 - accuracy: 0.9991\n",
            "Epoch 273: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0108 - accuracy: 0.9985 - val_loss: 0.2647 - val_accuracy: 0.9865\n",
            "Epoch 274/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0106 - accuracy: 0.9974\n",
            "Epoch 274: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0103 - accuracy: 0.9970 - val_loss: 0.2861 - val_accuracy: 0.9730\n",
            "Epoch 275/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0095 - accuracy: 0.9984\n",
            "Epoch 275: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0104 - accuracy: 0.9980 - val_loss: 0.2648 - val_accuracy: 0.9775\n",
            "Epoch 276/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0111 - accuracy: 0.9972\n",
            "Epoch 276: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0107 - accuracy: 0.9975 - val_loss: 0.2723 - val_accuracy: 0.9730\n",
            "Epoch 277/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9935\n",
            "Epoch 277: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0215 - accuracy: 0.9935 - val_loss: 0.3055 - val_accuracy: 0.9730\n",
            "Epoch 278/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0137 - accuracy: 0.9974\n",
            "Epoch 278: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0138 - accuracy: 0.9970 - val_loss: 0.2744 - val_accuracy: 0.9865\n",
            "Epoch 279/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0140 - accuracy: 0.9939\n",
            "Epoch 279: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0174 - accuracy: 0.9940 - val_loss: 0.2876 - val_accuracy: 0.9775\n",
            "Epoch 280/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0239 - accuracy: 0.9916\n",
            "Epoch 280: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0224 - accuracy: 0.9925 - val_loss: 0.3277 - val_accuracy: 0.9730\n",
            "Epoch 281/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0151 - accuracy: 0.9969\n",
            "Epoch 281: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0148 - accuracy: 0.9970 - val_loss: 0.3057 - val_accuracy: 0.9730\n",
            "Epoch 282/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0174 - accuracy: 0.9941\n",
            "Epoch 282: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0202 - accuracy: 0.9940 - val_loss: 0.3243 - val_accuracy: 0.9640\n",
            "Epoch 283/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0371 - accuracy: 0.9874\n",
            "Epoch 283: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0330 - accuracy: 0.9890 - val_loss: 0.4103 - val_accuracy: 0.9595\n",
            "Epoch 284/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0302 - accuracy: 0.9913\n",
            "Epoch 284: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0292 - accuracy: 0.9915 - val_loss: 0.3323 - val_accuracy: 0.9640\n",
            "Epoch 285/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9915\n",
            "Epoch 285: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0254 - accuracy: 0.9915 - val_loss: 0.2938 - val_accuracy: 0.9685\n",
            "Epoch 286/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0173 - accuracy: 0.9945\n",
            "Epoch 286: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0195 - accuracy: 0.9940 - val_loss: 0.3827 - val_accuracy: 0.9595\n",
            "Epoch 287/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0295 - accuracy: 0.9878\n",
            "Epoch 287: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0362 - accuracy: 0.9864 - val_loss: 0.2900 - val_accuracy: 0.9775\n",
            "Epoch 288/512\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 0.0447 - accuracy: 0.9874\n",
            "Epoch 288: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0476 - accuracy: 0.9859 - val_loss: 0.3284 - val_accuracy: 0.9685\n",
            "Epoch 289/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0414 - accuracy: 0.9864\n",
            "Epoch 289: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0414 - accuracy: 0.9864 - val_loss: 0.3473 - val_accuracy: 0.9685\n",
            "Epoch 290/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0443 - accuracy: 0.9865\n",
            "Epoch 290: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0477 - accuracy: 0.9859 - val_loss: 0.3160 - val_accuracy: 0.9730\n",
            "Epoch 291/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0707 - accuracy: 0.9786\n",
            "Epoch 291: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0743 - accuracy: 0.9769 - val_loss: 0.3513 - val_accuracy: 0.9505\n",
            "Epoch 292/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1163 - accuracy: 0.9583\n",
            "Epoch 292: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1163 - accuracy: 0.9583 - val_loss: 0.4030 - val_accuracy: 0.9279\n",
            "Epoch 293/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.1500 - accuracy: 0.9444\n",
            "Epoch 293: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1611 - accuracy: 0.9473 - val_loss: 0.4539 - val_accuracy: 0.9054\n",
            "Epoch 294/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1341 - accuracy: 0.9528\n",
            "Epoch 294: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1341 - accuracy: 0.9528 - val_loss: 0.4460 - val_accuracy: 0.9369\n",
            "Epoch 295/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0598 - accuracy: 0.9799\n",
            "Epoch 295: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0598 - accuracy: 0.9799 - val_loss: 0.3349 - val_accuracy: 0.9595\n",
            "Epoch 296/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0586 - accuracy: 0.9812\n",
            "Epoch 296: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0579 - accuracy: 0.9814 - val_loss: 0.3457 - val_accuracy: 0.9595\n",
            "Epoch 297/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0500 - accuracy: 0.9861\n",
            "Epoch 297: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0535 - accuracy: 0.9834 - val_loss: 0.3406 - val_accuracy: 0.9685\n",
            "Epoch 298/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0565 - accuracy: 0.9844\n",
            "Epoch 298: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0565 - accuracy: 0.9844 - val_loss: 0.3285 - val_accuracy: 0.9595\n",
            "Epoch 299/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0320 - accuracy: 0.9905\n",
            "Epoch 299: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0336 - accuracy: 0.9900 - val_loss: 0.2918 - val_accuracy: 0.9685\n",
            "Epoch 300/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0180 - accuracy: 0.9953\n",
            "Epoch 300: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0177 - accuracy: 0.9955 - val_loss: 0.2835 - val_accuracy: 0.9730\n",
            "Epoch 301/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0117 - accuracy: 0.9990\n",
            "Epoch 301: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0115 - accuracy: 0.9990 - val_loss: 0.2918 - val_accuracy: 0.9820\n",
            "Epoch 302/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0092 - accuracy: 0.9983\n",
            "Epoch 302: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0098 - accuracy: 0.9975 - val_loss: 0.2539 - val_accuracy: 0.9820\n",
            "Epoch 303/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0131 - accuracy: 0.9967\n",
            "Epoch 303: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0122 - accuracy: 0.9970 - val_loss: 0.3062 - val_accuracy: 0.9775\n",
            "Epoch 304/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0105 - accuracy: 0.9978\n",
            "Epoch 304: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0107 - accuracy: 0.9975 - val_loss: 0.3476 - val_accuracy: 0.9820\n",
            "Epoch 305/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0154 - accuracy: 0.9971\n",
            "Epoch 305: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0174 - accuracy: 0.9955 - val_loss: 0.3081 - val_accuracy: 0.9820\n",
            "Epoch 306/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0165 - accuracy: 0.9953\n",
            "Epoch 306: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0164 - accuracy: 0.9955 - val_loss: 0.3068 - val_accuracy: 0.9595\n",
            "Epoch 307/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0137 - accuracy: 0.9953\n",
            "Epoch 307: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0144 - accuracy: 0.9950 - val_loss: 0.2860 - val_accuracy: 0.9820\n",
            "Epoch 308/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0123 - accuracy: 0.9964\n",
            "Epoch 308: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0137 - accuracy: 0.9955 - val_loss: 0.2935 - val_accuracy: 0.9820\n",
            "Epoch 309/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0177 - accuracy: 0.9950\n",
            "Epoch 309: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0177 - accuracy: 0.9945 - val_loss: 0.2578 - val_accuracy: 0.9865\n",
            "Epoch 310/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0176 - accuracy: 0.9930\n",
            "Epoch 310: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0164 - accuracy: 0.9940 - val_loss: 0.2937 - val_accuracy: 0.9730\n",
            "Epoch 311/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0135 - accuracy: 0.9961\n",
            "Epoch 311: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0132 - accuracy: 0.9960 - val_loss: 0.2890 - val_accuracy: 0.9820\n",
            "Epoch 312/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0109 - accuracy: 0.9967\n",
            "Epoch 312: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0097 - accuracy: 0.9975 - val_loss: 0.2843 - val_accuracy: 0.9730\n",
            "Epoch 313/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0106 - accuracy: 0.9979\n",
            "Epoch 313: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0093 - accuracy: 0.9980 - val_loss: 0.3018 - val_accuracy: 0.9775\n",
            "Epoch 314/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0124 - accuracy: 0.9984\n",
            "Epoch 314: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0100 - accuracy: 0.9990 - val_loss: 0.2960 - val_accuracy: 0.9775\n",
            "Epoch 315/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0126 - accuracy: 0.9969\n",
            "Epoch 315: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0104 - accuracy: 0.9975 - val_loss: 0.3178 - val_accuracy: 0.9730\n",
            "Epoch 316/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0120 - accuracy: 0.9979\n",
            "Epoch 316: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0111 - accuracy: 0.9980 - val_loss: 0.2953 - val_accuracy: 0.9775\n",
            "Epoch 317/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0097 - accuracy: 0.9964\n",
            "Epoch 317: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0146 - accuracy: 0.9955 - val_loss: 0.2863 - val_accuracy: 0.9820\n",
            "Epoch 318/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0167 - accuracy: 0.9948\n",
            "Epoch 318: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0189 - accuracy: 0.9930 - val_loss: 0.2901 - val_accuracy: 0.9595\n",
            "Epoch 319/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0266 - accuracy: 0.9915\n",
            "Epoch 319: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0233 - accuracy: 0.9935 - val_loss: 0.2999 - val_accuracy: 0.9685\n",
            "Epoch 320/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0190 - accuracy: 0.9965\n",
            "Epoch 320: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0150 - accuracy: 0.9970 - val_loss: 0.2983 - val_accuracy: 0.9730\n",
            "Epoch 321/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0301 - accuracy: 0.9887\n",
            "Epoch 321: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0303 - accuracy: 0.9885 - val_loss: 0.3648 - val_accuracy: 0.9730\n",
            "Epoch 322/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0508 - accuracy: 0.9792\n",
            "Epoch 322: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0508 - accuracy: 0.9794 - val_loss: 0.3551 - val_accuracy: 0.9459\n",
            "Epoch 323/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0689 - accuracy: 0.9766\n",
            "Epoch 323: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0829 - accuracy: 0.9719 - val_loss: 0.3614 - val_accuracy: 0.9459\n",
            "Epoch 324/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0928 - accuracy: 0.9664\n",
            "Epoch 324: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0930 - accuracy: 0.9684 - val_loss: 0.3757 - val_accuracy: 0.9234\n",
            "Epoch 325/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0835 - accuracy: 0.9723\n",
            "Epoch 325: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0809 - accuracy: 0.9734 - val_loss: 0.3328 - val_accuracy: 0.9414\n",
            "Epoch 326/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0667 - accuracy: 0.9797\n",
            "Epoch 326: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0623 - accuracy: 0.9794 - val_loss: 0.3132 - val_accuracy: 0.9550\n",
            "Epoch 327/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0407 - accuracy: 0.9886\n",
            "Epoch 327: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0415 - accuracy: 0.9874 - val_loss: 0.3731 - val_accuracy: 0.9505\n",
            "Epoch 328/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0334 - accuracy: 0.9873\n",
            "Epoch 328: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0450 - accuracy: 0.9829 - val_loss: 0.2623 - val_accuracy: 0.9775\n",
            "Epoch 329/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0347 - accuracy: 0.9893\n",
            "Epoch 329: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0274 - accuracy: 0.9925 - val_loss: 0.2980 - val_accuracy: 0.9865\n",
            "Epoch 330/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0231 - accuracy: 0.9906\n",
            "Epoch 330: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0361 - accuracy: 0.9890 - val_loss: 0.3130 - val_accuracy: 0.9820\n",
            "Epoch 331/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0430 - accuracy: 0.9883\n",
            "Epoch 331: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0536 - accuracy: 0.9829 - val_loss: 0.3927 - val_accuracy: 0.9234\n",
            "Epoch 332/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0592 - accuracy: 0.9818\n",
            "Epoch 332: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0547 - accuracy: 0.9809 - val_loss: 0.2805 - val_accuracy: 0.9730\n",
            "Epoch 333/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0382 - accuracy: 0.9883\n",
            "Epoch 333: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0377 - accuracy: 0.9890 - val_loss: 0.3255 - val_accuracy: 0.9640\n",
            "Epoch 334/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0159 - accuracy: 0.9991\n",
            "Epoch 334: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0177 - accuracy: 0.9980 - val_loss: 0.2868 - val_accuracy: 0.9820\n",
            "Epoch 335/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0128 - accuracy: 0.9974\n",
            "Epoch 335: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0148 - accuracy: 0.9970 - val_loss: 0.2904 - val_accuracy: 0.9775\n",
            "Epoch 336/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 336: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.2815 - val_accuracy: 0.9775\n",
            "Epoch 337/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0071 - accuracy: 0.9979\n",
            "Epoch 337: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.3080 - val_accuracy: 0.9820\n",
            "Epoch 338/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 338: val_accuracy did not improve from 0.98649\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0056 - accuracy: 0.9995 - val_loss: 0.2926 - val_accuracy: 0.9775\n",
            "Epoch 339/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 339: val_accuracy improved from 0.98649 to 0.99099, saving model to model.h5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.2773 - val_accuracy: 0.9910\n",
            "Epoch 340/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0052 - accuracy: 0.9987\n",
            "Epoch 340: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 0.3004 - val_accuracy: 0.9820\n",
            "Epoch 341/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 341: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0045 - accuracy: 0.9995 - val_loss: 0.2924 - val_accuracy: 0.9820\n",
            "Epoch 342/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0055 - accuracy: 0.9986\n",
            "Epoch 342: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0080 - accuracy: 0.9975 - val_loss: 0.3206 - val_accuracy: 0.9820\n",
            "Epoch 343/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0313 - accuracy: 0.9937\n",
            "Epoch 343: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0302 - accuracy: 0.9930 - val_loss: 0.2945 - val_accuracy: 0.9730\n",
            "Epoch 344/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0412 - accuracy: 0.9879\n",
            "Epoch 344: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0510 - accuracy: 0.9829 - val_loss: 0.3893 - val_accuracy: 0.9459\n",
            "Epoch 345/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0822 - accuracy: 0.9716\n",
            "Epoch 345: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0960 - accuracy: 0.9649 - val_loss: 0.3888 - val_accuracy: 0.9414\n",
            "Epoch 346/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.1233 - accuracy: 0.9553\n",
            "Epoch 346: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1077 - accuracy: 0.9598 - val_loss: 0.3820 - val_accuracy: 0.9189\n",
            "Epoch 347/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0787 - accuracy: 0.9822\n",
            "Epoch 347: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0765 - accuracy: 0.9814 - val_loss: 0.3508 - val_accuracy: 0.9505\n",
            "Epoch 348/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0764 - accuracy: 0.9774\n",
            "Epoch 348: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0808 - accuracy: 0.9774 - val_loss: 0.3257 - val_accuracy: 0.9595\n",
            "Epoch 349/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0656 - accuracy: 0.9811\n",
            "Epoch 349: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0569 - accuracy: 0.9839 - val_loss: 0.2462 - val_accuracy: 0.9775\n",
            "Epoch 350/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0317 - accuracy: 0.9905\n",
            "Epoch 350: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0333 - accuracy: 0.9895 - val_loss: 0.2690 - val_accuracy: 0.9685\n",
            "Epoch 351/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0310 - accuracy: 0.9915\n",
            "Epoch 351: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0279 - accuracy: 0.9925 - val_loss: 0.2922 - val_accuracy: 0.9820\n",
            "Epoch 352/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0147 - accuracy: 0.9977\n",
            "Epoch 352: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0139 - accuracy: 0.9980 - val_loss: 0.2659 - val_accuracy: 0.9910\n",
            "Epoch 353/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0110 - accuracy: 0.9969\n",
            "Epoch 353: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0088 - accuracy: 0.9980 - val_loss: 0.2856 - val_accuracy: 0.9820\n",
            "Epoch 354/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0069 - accuracy: 0.9980\n",
            "Epoch 354: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.2781 - val_accuracy: 0.9820\n",
            "Epoch 355/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 355: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0043 - accuracy: 0.9995 - val_loss: 0.2726 - val_accuracy: 0.9820\n",
            "Epoch 356/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 356: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.2651 - val_accuracy: 0.9820\n",
            "Epoch 357/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 357: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.2883 - val_accuracy: 0.9820\n",
            "Epoch 358/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 358: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.2714 - val_accuracy: 0.9820\n",
            "Epoch 359/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 359: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.2671 - val_accuracy: 0.9820\n",
            "Epoch 360/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0033 - accuracy: 0.9992\n",
            "Epoch 360: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.2878 - val_accuracy: 0.9820\n",
            "Epoch 361/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 361: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.2715 - val_accuracy: 0.9910\n",
            "Epoch 362/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 362: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.2771 - val_accuracy: 0.9820\n",
            "Epoch 363/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0026 - accuracy: 0.9995\n",
            "Epoch 363: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.2954 - val_accuracy: 0.9820\n",
            "Epoch 364/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 364: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2728 - val_accuracy: 0.9910\n",
            "Epoch 365/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 365: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.2937 - val_accuracy: 0.9820\n",
            "Epoch 366/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0022 - accuracy: 0.9995\n",
            "Epoch 366: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.2908 - val_accuracy: 0.9820\n",
            "Epoch 367/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 367: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2853 - val_accuracy: 0.9820\n",
            "Epoch 368/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000    \n",
            "Epoch 368: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.2902 - val_accuracy: 0.9820\n",
            "Epoch 369/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0053 - accuracy: 0.9984\n",
            "Epoch 369: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0074 - accuracy: 0.9980 - val_loss: 0.3002 - val_accuracy: 0.9865\n",
            "Epoch 370/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0108 - accuracy: 0.9974\n",
            "Epoch 370: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0114 - accuracy: 0.9970 - val_loss: 0.3434 - val_accuracy: 0.9775\n",
            "Epoch 371/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0164 - accuracy: 0.9939\n",
            "Epoch 371: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0127 - accuracy: 0.9960 - val_loss: 0.3183 - val_accuracy: 0.9640\n",
            "Epoch 372/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0069 - accuracy: 0.9972\n",
            "Epoch 372: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0078 - accuracy: 0.9970 - val_loss: 0.3115 - val_accuracy: 0.9820\n",
            "Epoch 373/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 373: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0052 - accuracy: 0.9985 - val_loss: 0.3019 - val_accuracy: 0.9820\n",
            "Epoch 374/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0087 - accuracy: 0.9969\n",
            "Epoch 374: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0085 - accuracy: 0.9970 - val_loss: 0.3202 - val_accuracy: 0.9820\n",
            "Epoch 375/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9980\n",
            "Epoch 375: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 0.3050 - val_accuracy: 0.9775\n",
            "Epoch 376/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.9970\n",
            "Epoch 376: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0085 - accuracy: 0.9970 - val_loss: 0.3221 - val_accuracy: 0.9775\n",
            "Epoch 377/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0046 - accuracy: 0.9985\n",
            "Epoch 377: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0046 - accuracy: 0.9985 - val_loss: 0.3731 - val_accuracy: 0.9730\n",
            "Epoch 378/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0359 - accuracy: 0.9885\n",
            "Epoch 378: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0366 - accuracy: 0.9885 - val_loss: 0.2731 - val_accuracy: 0.9595\n",
            "Epoch 379/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0663 - accuracy: 0.9750\n",
            "Epoch 379: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0643 - accuracy: 0.9759 - val_loss: 0.3011 - val_accuracy: 0.9550\n",
            "Epoch 380/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.1194 - accuracy: 0.9635\n",
            "Epoch 380: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1168 - accuracy: 0.9639 - val_loss: 0.3532 - val_accuracy: 0.9550\n",
            "Epoch 381/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0614 - accuracy: 0.9773\n",
            "Epoch 381: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1041 - accuracy: 0.9709 - val_loss: 0.4686 - val_accuracy: 0.9054\n",
            "Epoch 382/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2686 - accuracy: 0.9187\n",
            "Epoch 382: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.2686 - accuracy: 0.9187 - val_loss: 0.4963 - val_accuracy: 0.8874\n",
            "Epoch 383/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.2327 - accuracy: 0.9175\n",
            "Epoch 383: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2151 - accuracy: 0.9167 - val_loss: 0.4036 - val_accuracy: 0.8829\n",
            "Epoch 384/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.1204 - accuracy: 0.9574\n",
            "Epoch 384: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1235 - accuracy: 0.9543 - val_loss: 0.3277 - val_accuracy: 0.9369\n",
            "Epoch 385/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.1048 - accuracy: 0.9583\n",
            "Epoch 385: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0957 - accuracy: 0.9618 - val_loss: 0.3744 - val_accuracy: 0.9234\n",
            "Epoch 386/512\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 0.0749 - accuracy: 0.9759\n",
            "Epoch 386: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0728 - accuracy: 0.9769 - val_loss: 0.2906 - val_accuracy: 0.9505\n",
            "Epoch 387/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0450 - accuracy: 0.9812\n",
            "Epoch 387: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0411 - accuracy: 0.9834 - val_loss: 0.2794 - val_accuracy: 0.9730\n",
            "Epoch 388/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0241 - accuracy: 0.9969\n",
            "Epoch 388: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0242 - accuracy: 0.9950 - val_loss: 0.2194 - val_accuracy: 0.9775\n",
            "Epoch 389/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9960\n",
            "Epoch 389: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0180 - accuracy: 0.9960 - val_loss: 0.2599 - val_accuracy: 0.9820\n",
            "Epoch 390/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0113 - accuracy: 0.9974\n",
            "Epoch 390: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0126 - accuracy: 0.9965 - val_loss: 0.2430 - val_accuracy: 0.9775\n",
            "Epoch 391/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0118 - accuracy: 0.9974\n",
            "Epoch 391: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0133 - accuracy: 0.9975 - val_loss: 0.2497 - val_accuracy: 0.9775\n",
            "Epoch 392/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0123 - accuracy: 0.9971\n",
            "Epoch 392: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.2445 - val_accuracy: 0.9775\n",
            "Epoch 393/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0072 - accuracy: 0.9992\n",
            "Epoch 393: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0061 - accuracy: 0.9995 - val_loss: 0.2755 - val_accuracy: 0.9820\n",
            "Epoch 394/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0042 - accuracy: 0.9991\n",
            "Epoch 394: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0046 - accuracy: 0.9990 - val_loss: 0.2582 - val_accuracy: 0.9820\n",
            "Epoch 395/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 395: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.2546 - val_accuracy: 0.9820\n",
            "Epoch 396/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 396: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.2507 - val_accuracy: 0.9820\n",
            "Epoch 397/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0030 - accuracy: 0.9992\n",
            "Epoch 397: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.2674 - val_accuracy: 0.9820\n",
            "Epoch 398/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 398: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.2821 - val_accuracy: 0.9820\n",
            "Epoch 399/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 399: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2574 - val_accuracy: 0.9820\n",
            "Epoch 400/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 0.9995\n",
            "Epoch 400: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.2707 - val_accuracy: 0.9820\n",
            "Epoch 401/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 401: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.2708 - val_accuracy: 0.9820\n",
            "Epoch 402/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 402: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.2802 - val_accuracy: 0.9820\n",
            "Epoch 403/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0025 - accuracy: 0.9992\n",
            "Epoch 403: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.2717 - val_accuracy: 0.9820\n",
            "Epoch 404/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0022 - accuracy: 0.9991\n",
            "Epoch 404: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0026 - accuracy: 0.9990 - val_loss: 0.2787 - val_accuracy: 0.9820\n",
            "Epoch 405/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 405: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.2891 - val_accuracy: 0.9820\n",
            "Epoch 406/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0026 - accuracy: 0.9991\n",
            "Epoch 406: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0026 - accuracy: 0.9990 - val_loss: 0.2827 - val_accuracy: 0.9820\n",
            "Epoch 407/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 407: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.2829 - val_accuracy: 0.9820\n",
            "Epoch 408/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 408: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.2817 - val_accuracy: 0.9820\n",
            "Epoch 409/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0035 - accuracy: 0.9993\n",
            "Epoch 409: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.3223 - val_accuracy: 0.9775\n",
            "Epoch 410/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0094 - accuracy: 0.9986\n",
            "Epoch 410: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0081 - accuracy: 0.9990 - val_loss: 0.2733 - val_accuracy: 0.9775\n",
            "Epoch 411/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0067 - accuracy: 0.9984\n",
            "Epoch 411: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0100 - accuracy: 0.9975 - val_loss: 0.2797 - val_accuracy: 0.9820\n",
            "Epoch 412/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0092 - accuracy: 0.9957\n",
            "Epoch 412: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0091 - accuracy: 0.9965 - val_loss: 0.2940 - val_accuracy: 0.9820\n",
            "Epoch 413/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0276 - accuracy: 0.9931\n",
            "Epoch 413: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0202 - accuracy: 0.9955 - val_loss: 0.2983 - val_accuracy: 0.9820\n",
            "Epoch 414/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0112 - accuracy: 0.9974\n",
            "Epoch 414: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0111 - accuracy: 0.9965 - val_loss: 0.2559 - val_accuracy: 0.9865\n",
            "Epoch 415/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0150 - accuracy: 0.9945\n",
            "Epoch 415: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0150 - accuracy: 0.9945 - val_loss: 0.2791 - val_accuracy: 0.9820\n",
            "Epoch 416/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0072 - accuracy: 0.9992\n",
            "Epoch 416: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0076 - accuracy: 0.9990 - val_loss: 0.2940 - val_accuracy: 0.9820\n",
            "Epoch 417/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0074 - accuracy: 0.9977\n",
            "Epoch 417: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0071 - accuracy: 0.9980 - val_loss: 0.2658 - val_accuracy: 0.9820\n",
            "Epoch 418/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0036 - accuracy: 0.9992\n",
            "Epoch 418: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0033 - accuracy: 0.9995 - val_loss: 0.2833 - val_accuracy: 0.9775\n",
            "Epoch 419/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0050 - accuracy: 0.9992\n",
            "Epoch 419: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0043 - accuracy: 0.9995 - val_loss: 0.2824 - val_accuracy: 0.9820\n",
            "Epoch 420/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0031 - accuracy: 0.9993\n",
            "Epoch 420: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.2875 - val_accuracy: 0.9820\n",
            "Epoch 421/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0024 - accuracy: 0.9993\n",
            "Epoch 421: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0033 - accuracy: 0.9985 - val_loss: 0.2749 - val_accuracy: 0.9775\n",
            "Epoch 422/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0098 - accuracy: 0.9961\n",
            "Epoch 422: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0072 - accuracy: 0.9980 - val_loss: 0.2624 - val_accuracy: 0.9820\n",
            "Epoch 423/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0052 - accuracy: 0.9983\n",
            "Epoch 423: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0054 - accuracy: 0.9980 - val_loss: 0.3208 - val_accuracy: 0.9820\n",
            "Epoch 424/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0055 - accuracy: 0.9983\n",
            "Epoch 424: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0049 - accuracy: 0.9990 - val_loss: 0.2662 - val_accuracy: 0.9865\n",
            "Epoch 425/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0061 - accuracy: 0.9984\n",
            "Epoch 425: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.3263 - val_accuracy: 0.9730\n",
            "Epoch 426/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0061 - accuracy: 0.9986\n",
            "Epoch 426: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0058 - accuracy: 0.9985 - val_loss: 0.3032 - val_accuracy: 0.9820\n",
            "Epoch 427/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 427: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2799 - val_accuracy: 0.9820\n",
            "Epoch 428/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 428: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.2707 - val_accuracy: 0.9820\n",
            "Epoch 429/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000    \n",
            "Epoch 429: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.3092 - val_accuracy: 0.9820\n",
            "Epoch 430/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000    \n",
            "Epoch 430: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2960 - val_accuracy: 0.9820\n",
            "Epoch 431/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000    \n",
            "Epoch 431: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.2902 - val_accuracy: 0.9820\n",
            "Epoch 432/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 432: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 9.7242e-04 - accuracy: 1.0000 - val_loss: 0.3001 - val_accuracy: 0.9820\n",
            "Epoch 433/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9995    \n",
            "Epoch 433: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0012 - accuracy: 0.9995 - val_loss: 0.2922 - val_accuracy: 0.9820\n",
            "Epoch 434/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 434: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3023 - val_accuracy: 0.9820\n",
            "Epoch 435/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 8.0756e-04 - accuracy: 1.0000\n",
            "Epoch 435: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2917 - val_accuracy: 0.9820\n",
            "Epoch 436/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000    \n",
            "Epoch 436: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3145 - val_accuracy: 0.9820\n",
            "Epoch 437/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 437: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2938 - val_accuracy: 0.9820\n",
            "Epoch 438/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 438: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.2849 - val_accuracy: 0.9820\n",
            "Epoch 439/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0066 - accuracy: 0.9971    \n",
            "Epoch 439: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0091 - accuracy: 0.9970 - val_loss: 0.2816 - val_accuracy: 0.9820\n",
            "Epoch 440/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0169 - accuracy: 0.9953\n",
            "Epoch 440: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0160 - accuracy: 0.9960 - val_loss: 0.3139 - val_accuracy: 0.9820\n",
            "Epoch 441/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0901 - accuracy: 0.9754\n",
            "Epoch 441: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0970 - accuracy: 0.9744 - val_loss: 0.2943 - val_accuracy: 0.9414\n",
            "Epoch 442/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.1923 - accuracy: 0.9469\n",
            "Epoch 442: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1949 - accuracy: 0.9463 - val_loss: 0.5294 - val_accuracy: 0.8604\n",
            "Epoch 443/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.3411 - accuracy: 0.8809\n",
            "Epoch 443: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3543 - accuracy: 0.8720 - val_loss: 0.3603 - val_accuracy: 0.9009\n",
            "Epoch 444/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.2360 - accuracy: 0.9031\n",
            "Epoch 444: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.2269 - accuracy: 0.9056 - val_loss: 0.3226 - val_accuracy: 0.9054\n",
            "Epoch 445/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.1404 - accuracy: 0.9462\n",
            "Epoch 445: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1480 - accuracy: 0.9458 - val_loss: 0.2632 - val_accuracy: 0.9234\n",
            "Epoch 446/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.1305 - accuracy: 0.9514\n",
            "Epoch 446: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1162 - accuracy: 0.9568 - val_loss: 0.2570 - val_accuracy: 0.9595\n",
            "Epoch 447/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0583 - accuracy: 0.9844\n",
            "Epoch 447: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0575 - accuracy: 0.9819 - val_loss: 0.2435 - val_accuracy: 0.9550\n",
            "Epoch 448/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0423 - accuracy: 0.9917\n",
            "Epoch 448: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0412 - accuracy: 0.9920 - val_loss: 0.2459 - val_accuracy: 0.9640\n",
            "Epoch 449/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0326 - accuracy: 0.9916\n",
            "Epoch 449: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0322 - accuracy: 0.9920 - val_loss: 0.2706 - val_accuracy: 0.9640\n",
            "Epoch 450/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0319 - accuracy: 0.9913\n",
            "Epoch 450: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0288 - accuracy: 0.9940 - val_loss: 0.2529 - val_accuracy: 0.9685\n",
            "Epoch 451/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9955\n",
            "Epoch 451: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0163 - accuracy: 0.9955 - val_loss: 0.2090 - val_accuracy: 0.9820\n",
            "Epoch 452/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9970\n",
            "Epoch 452: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0123 - accuracy: 0.9970 - val_loss: 0.2071 - val_accuracy: 0.9685\n",
            "Epoch 453/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 0.9985\n",
            "Epoch 453: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0097 - accuracy: 0.9985 - val_loss: 0.1874 - val_accuracy: 0.9865\n",
            "Epoch 454/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0104 - accuracy: 0.9991\n",
            "Epoch 454: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0100 - accuracy: 0.9990 - val_loss: 0.1952 - val_accuracy: 0.9775\n",
            "Epoch 455/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0382 - accuracy: 0.9880\n",
            "Epoch 455: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0390 - accuracy: 0.9874 - val_loss: 0.2899 - val_accuracy: 0.9730\n",
            "Epoch 456/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0890 - accuracy: 0.9714\n",
            "Epoch 456: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0899 - accuracy: 0.9709 - val_loss: 0.4246 - val_accuracy: 0.9505\n",
            "Epoch 457/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0997 - accuracy: 0.9684\n",
            "Epoch 457: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0997 - accuracy: 0.9684 - val_loss: 0.2712 - val_accuracy: 0.9640\n",
            "Epoch 458/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0479 - accuracy: 0.9844\n",
            "Epoch 458: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0475 - accuracy: 0.9844 - val_loss: 0.2669 - val_accuracy: 0.9730\n",
            "Epoch 459/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0309 - accuracy: 0.9932\n",
            "Epoch 459: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0295 - accuracy: 0.9920 - val_loss: 0.2936 - val_accuracy: 0.9505\n",
            "Epoch 460/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0291 - accuracy: 0.9878\n",
            "Epoch 460: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0259 - accuracy: 0.9900 - val_loss: 0.2510 - val_accuracy: 0.9775\n",
            "Epoch 461/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0198 - accuracy: 0.9937\n",
            "Epoch 461: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0192 - accuracy: 0.9940 - val_loss: 0.2515 - val_accuracy: 0.9730\n",
            "Epoch 462/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0174 - accuracy: 0.9977\n",
            "Epoch 462: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0308 - accuracy: 0.9945 - val_loss: 0.2579 - val_accuracy: 0.9685\n",
            "Epoch 463/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0189 - accuracy: 0.9957\n",
            "Epoch 463: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0167 - accuracy: 0.9960 - val_loss: 0.2726 - val_accuracy: 0.9775\n",
            "Epoch 464/512\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 0.0176 - accuracy: 0.9950\n",
            "Epoch 464: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0177 - accuracy: 0.9950 - val_loss: 0.2572 - val_accuracy: 0.9775\n",
            "Epoch 465/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0123 - accuracy: 0.9957\n",
            "Epoch 465: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0172 - accuracy: 0.9945 - val_loss: 0.2454 - val_accuracy: 0.9730\n",
            "Epoch 466/512\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.0451 - accuracy: 0.9854\n",
            "Epoch 466: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0463 - accuracy: 0.9849 - val_loss: 0.2338 - val_accuracy: 0.9640\n",
            "Epoch 467/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0369 - accuracy: 0.9906\n",
            "Epoch 467: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0483 - accuracy: 0.9869 - val_loss: 0.2391 - val_accuracy: 0.9685\n",
            "Epoch 468/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0472 - accuracy: 0.9818\n",
            "Epoch 468: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0424 - accuracy: 0.9849 - val_loss: 0.2214 - val_accuracy: 0.9730\n",
            "Epoch 469/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0244 - accuracy: 0.9931\n",
            "Epoch 469: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.0232 - accuracy: 0.9950 - val_loss: 0.2461 - val_accuracy: 0.9730\n",
            "Epoch 470/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0142 - accuracy: 0.9957\n",
            "Epoch 470: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0139 - accuracy: 0.9965 - val_loss: 0.2268 - val_accuracy: 0.9820\n",
            "Epoch 471/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0066 - accuracy: 0.9992\n",
            "Epoch 471: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0069 - accuracy: 0.9990 - val_loss: 0.2599 - val_accuracy: 0.9820\n",
            "Epoch 472/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 0.9985\n",
            "Epoch 472: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.2218 - val_accuracy: 0.9775\n",
            "Epoch 473/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0052 - accuracy: 0.9991\n",
            "Epoch 473: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0047 - accuracy: 0.9995 - val_loss: 0.2449 - val_accuracy: 0.9820\n",
            "Epoch 474/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 474: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.2408 - val_accuracy: 0.9775\n",
            "Epoch 475/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0051 - accuracy: 0.9984\n",
            "Epoch 475: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.2386 - val_accuracy: 0.9820\n",
            "Epoch 476/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 476: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.2428 - val_accuracy: 0.9820\n",
            "Epoch 477/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0369 - accuracy: 0.9883\n",
            "Epoch 477: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0361 - accuracy: 0.9890 - val_loss: 0.2594 - val_accuracy: 0.9730\n",
            "Epoch 478/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0261 - accuracy: 0.9908\n",
            "Epoch 478: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0361 - accuracy: 0.9900 - val_loss: 0.2979 - val_accuracy: 0.9595\n",
            "Epoch 479/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0701 - accuracy: 0.9766\n",
            "Epoch 479: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0779 - accuracy: 0.9764 - val_loss: 0.4596 - val_accuracy: 0.9324\n",
            "Epoch 480/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.1425 - accuracy: 0.9648\n",
            "Epoch 480: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1108 - accuracy: 0.9699 - val_loss: 0.2849 - val_accuracy: 0.9459\n",
            "Epoch 481/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0653 - accuracy: 0.9750\n",
            "Epoch 481: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0597 - accuracy: 0.9789 - val_loss: 0.2695 - val_accuracy: 0.9640\n",
            "Epoch 482/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0462 - accuracy: 0.9859\n",
            "Epoch 482: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0521 - accuracy: 0.9839 - val_loss: 0.2877 - val_accuracy: 0.9820\n",
            "Epoch 483/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0339 - accuracy: 0.9891\n",
            "Epoch 483: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0308 - accuracy: 0.9910 - val_loss: 0.3294 - val_accuracy: 0.9730\n",
            "Epoch 484/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0282 - accuracy: 0.9939\n",
            "Epoch 484: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0232 - accuracy: 0.9935 - val_loss: 0.2825 - val_accuracy: 0.9730\n",
            "Epoch 485/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0277 - accuracy: 0.9893\n",
            "Epoch 485: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0254 - accuracy: 0.9915 - val_loss: 0.3039 - val_accuracy: 0.9685\n",
            "Epoch 486/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0156 - accuracy: 0.9961\n",
            "Epoch 486: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0144 - accuracy: 0.9970 - val_loss: 0.3108 - val_accuracy: 0.9730\n",
            "Epoch 487/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0114 - accuracy: 0.9984\n",
            "Epoch 487: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0103 - accuracy: 0.9985 - val_loss: 0.3322 - val_accuracy: 0.9775\n",
            "Epoch 488/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0096 - accuracy: 0.9992\n",
            "Epoch 488: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0079 - accuracy: 0.9995 - val_loss: 0.2835 - val_accuracy: 0.9820\n",
            "Epoch 489/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0040 - accuracy: 0.9991\n",
            "Epoch 489: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0035 - accuracy: 0.9995 - val_loss: 0.2868 - val_accuracy: 0.9820\n",
            "Epoch 490/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 490: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.2961 - val_accuracy: 0.9820\n",
            "Epoch 491/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 491: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.2918 - val_accuracy: 0.9820\n",
            "Epoch 492/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 492: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.3005 - val_accuracy: 0.9820\n",
            "Epoch 493/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 493: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2955 - val_accuracy: 0.9820\n",
            "Epoch 494/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000    \n",
            "Epoch 494: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.2884 - val_accuracy: 0.9820\n",
            "Epoch 495/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000    \n",
            "Epoch 495: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2951 - val_accuracy: 0.9820\n",
            "Epoch 496/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 496: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2927 - val_accuracy: 0.9820\n",
            "Epoch 497/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 497: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2933 - val_accuracy: 0.9820\n",
            "Epoch 498/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000    \n",
            "Epoch 498: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2951 - val_accuracy: 0.9820\n",
            "Epoch 499/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 499: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2972 - val_accuracy: 0.9820\n",
            "Epoch 500/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000    \n",
            "Epoch 500: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2970 - val_accuracy: 0.9820\n",
            "Epoch 501/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000    \n",
            "Epoch 501: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3014 - val_accuracy: 0.9820\n",
            "Epoch 502/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 502: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2977 - val_accuracy: 0.9820\n",
            "Epoch 503/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000    \n",
            "Epoch 503: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 9.2666e-04 - accuracy: 1.0000 - val_loss: 0.2963 - val_accuracy: 0.9820\n",
            "Epoch 504/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 8.5339e-04 - accuracy: 1.0000\n",
            "Epoch 504: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 9.0623e-04 - accuracy: 1.0000 - val_loss: 0.2980 - val_accuracy: 0.9820\n",
            "Epoch 505/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 7.3333e-04 - accuracy: 1.0000\n",
            "Epoch 505: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3059 - val_accuracy: 0.9820\n",
            "Epoch 506/512\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000    \n",
            "Epoch 506: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.2960 - val_accuracy: 0.9820\n",
            "Epoch 507/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 9.9887e-04 - accuracy: 1.0000\n",
            "Epoch 507: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 9.6387e-04 - accuracy: 1.0000 - val_loss: 0.3047 - val_accuracy: 0.9820\n",
            "Epoch 508/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 508: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3026 - val_accuracy: 0.9820\n",
            "Epoch 509/512\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 7.2198e-04 - accuracy: 1.0000\n",
            "Epoch 509: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 9.4935e-04 - accuracy: 1.0000 - val_loss: 0.2998 - val_accuracy: 0.9820\n",
            "Epoch 510/512\n",
            "10/16 [=================>............] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000    \n",
            "Epoch 510: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 9.6215e-04 - accuracy: 1.0000 - val_loss: 0.3030 - val_accuracy: 0.9820\n",
            "Epoch 511/512\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 0.0016 - accuracy: 0.9990    \n",
            "Epoch 511: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.3028 - val_accuracy: 0.9820\n",
            "Epoch 512/512\n",
            "16/16 [==============================] - ETA: 0s - loss: 6.7032e-04 - accuracy: 1.0000\n",
            "Epoch 512: val_accuracy did not improve from 0.99099\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 6.7032e-04 - accuracy: 1.0000 - val_loss: 0.3015 - val_accuracy: 0.9820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 962
        },
        "id": "1x-UR-Ccla1i",
        "outputId": "3b8d508a-d7e4-490c-c047-4e58fe4b7fd9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = None\n",
        "model = tf.keras.models.load_model('model.h5')"
      ],
      "metadata": {
        "id": "EZpGQoWgjtkW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {(test_accuracy * 100):.4f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwubz27aaEub",
        "outputId": "1e671add-3b9e-4544-bff4-01d6f1b1ffd1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1320 - accuracy: 0.9896\n",
            "Test Accuracy: 98.9612%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"model.h5\")"
      ],
      "metadata": {
        "id": "idK0jeSja6lt"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}